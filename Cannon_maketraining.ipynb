{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compatibility with Python 3\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "try:\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format='retina'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Basic packages\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import astropy.io.fits as pyfits\n",
    "import astropy.table as table\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Matplotlib adjustments (you might not need all of these)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.cm as cmx\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "matplotlib.rc('text', usetex = True)\n",
    "params = {'text.latex.preamble': [r'\\usepackage{upgreek}', r'\\usepackage{amsmath}'],'font.family' : 'lmodern','font.size' : 11}   \n",
    "plt.rcParams.update(params)\n",
    "\n",
    "willi_blau = [0.0722666667, 0.4886666667, 0.8467]\n",
    "\n",
    "# PARULA MAP\n",
    "_parula_data = [[0.2081, 0.1663, 0.5292], \n",
    "                [0.2116238095, 0.1897809524, 0.5776761905], \n",
    "                [0.212252381, 0.2137714286, 0.6269714286], \n",
    "                [0.2081, 0.2386, 0.6770857143], \n",
    "                [0.1959047619, 0.2644571429, 0.7279], \n",
    "                [0.1707285714, 0.2919380952, 0.779247619], \n",
    "                [0.1252714286, 0.3242428571, 0.8302714286], \n",
    "                [0.0591333333, 0.3598333333, 0.8683333333], \n",
    "                [0.0116952381, 0.3875095238, 0.8819571429], \n",
    "                [0.0059571429, 0.4086142857, 0.8828428571], \n",
    "                [0.0165142857, 0.4266, 0.8786333333], \n",
    "                [0.032852381, 0.4430428571, 0.8719571429], \n",
    "                [0.0498142857, 0.4585714286, 0.8640571429], \n",
    "                [0.0629333333, 0.4736904762, 0.8554380952], \n",
    "                [0.0722666667, 0.4886666667, 0.8467], \n",
    "                [0.0779428571, 0.5039857143, 0.8383714286], \n",
    "                [0.079347619, 0.5200238095, 0.8311809524], \n",
    "                [0.0749428571, 0.5375428571, 0.8262714286], \n",
    "                [0.0640571429, 0.5569857143, 0.8239571429], \n",
    "                [0.0487714286, 0.5772238095, 0.8228285714], \n",
    "                [0.0343428571, 0.5965809524, 0.819852381], \n",
    "                [0.0265, 0.6137, 0.8135], \n",
    "                [0.0238904762, 0.6286619048, 0.8037619048], \n",
    "                [0.0230904762, 0.6417857143, 0.7912666667], \n",
    "                [0.0227714286, 0.6534857143, 0.7767571429], \n",
    "                [0.0266619048, 0.6641952381, 0.7607190476], \n",
    "                [0.0383714286, 0.6742714286, 0.743552381], \n",
    "                [0.0589714286, 0.6837571429, 0.7253857143], \n",
    "                [0.0843, 0.6928333333, 0.7061666667], \n",
    "                [0.1132952381, 0.7015, 0.6858571429], \n",
    "                [0.1452714286, 0.7097571429, 0.6646285714], \n",
    "                [0.1801333333, 0.7176571429, 0.6424333333], \n",
    "                [0.2178285714, 0.7250428571, 0.6192619048], \n",
    "                [0.2586428571, 0.7317142857, 0.5954285714], \n",
    "                [0.3021714286, 0.7376047619, 0.5711857143], \n",
    "                [0.3481666667, 0.7424333333, 0.5472666667], \n",
    "                [0.3952571429, 0.7459, 0.5244428571], \n",
    "                [0.4420095238, 0.7480809524, 0.5033142857], \n",
    "                [0.4871238095, 0.7490619048, 0.4839761905], \n",
    "                [0.5300285714, 0.7491142857, 0.4661142857], \n",
    "                [0.5708571429, 0.7485190476, 0.4493904762],\n",
    "                [0.609852381, 0.7473142857, 0.4336857143], \n",
    "                [0.6473, 0.7456, 0.4188], \n",
    "                [0.6834190476, 0.7434761905, 0.4044333333], \n",
    "                [0.7184095238, 0.7411333333, 0.3904761905], \n",
    "                [0.7524857143, 0.7384, 0.3768142857], \n",
    "                [0.7858428571, 0.7355666667, 0.3632714286], \n",
    "                [0.8185047619, 0.7327333333, 0.3497904762], \n",
    "                [0.8506571429, 0.7299, 0.3360285714], \n",
    "                [0.8824333333, 0.7274333333, 0.3217], \n",
    "                [0.9139333333, 0.7257857143, 0.3062761905], \n",
    "                [0.9449571429, 0.7261142857, 0.2886428571], \n",
    "                [0.9738952381, 0.7313952381, 0.266647619], \n",
    "                [0.9937714286, 0.7454571429, 0.240347619], \n",
    "                [0.9990428571, 0.7653142857, 0.2164142857], \n",
    "                [0.9955333333, 0.7860571429, 0.196652381], \n",
    "                [0.988, 0.8066, 0.1793666667], \n",
    "                [0.9788571429, 0.8271428571, 0.1633142857], \n",
    "                [0.9697, 0.8481380952, 0.147452381], \n",
    "                [0.9625857143, 0.8705142857, 0.1309], \n",
    "                [0.9588714286, 0.8949, 0.1132428571], \n",
    "                [0.9598238095, 0.9218333333, 0.0948380952], \n",
    "                [0.9661, 0.9514428571, 0.0755333333], \n",
    "                [0.9763, 0.9831, 0.0538]]\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "parula = ListedColormap(_parula_data, name='parula')\n",
    "parula_zero = _parula_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tex_dict = dict(\n",
    "    wave_ccd1 = r'$\\lambda$ (CCD1) [$\\mathrm{\\AA}$]',\n",
    "    wave_ccd2 = r'$\\lambda$ (CCD2) [$\\mathrm{\\AA}$]',\n",
    "    wave_ccd3 = r'$\\lambda$ (CCD3) [$\\mathrm{\\AA}$]',\n",
    "    wave_ccd4 = r'$\\lambda$ (CCD4) [$\\mathrm{\\AA}$]',\n",
    "    wave      = r'$\\lambda$ [$\\mathrm{\\AA}$]',\n",
    "    norm_flux = r'$\\mathrm{Flux~[norm]}$',\n",
    "    Teff      = r'$T_\\mathrm{eff}~\\mathrm{[K]}$',\n",
    "    Logg      = r'$\\log g~\\mathrm{[dex]}$',\n",
    "    Fe_H      = r'$\\mathrm{[Fe/H]~[dex]}$',\n",
    "    M_H       = r'$\\mathrm{[M/H]~[dex]}$',\n",
    "    Vmic      = r'$v_\\mathrm{mic}~\\mathrm{[km/s]}$',\n",
    "    Vsini     = r'$v_\\mathrm{broad}~\\mathrm{[km/s]}$',\n",
    "    Ak        = r'$A_{K_S}~\\mathrm{[mag]}$',\n",
    "    Alpha_Fe  = r'$\\mathrm{[\\alpha/Fe]~[dex]}$',\n",
    "    XFe       = r'$\\mathrm{[X/Fe]~[dex]}$',\n",
    "    SNR       = r'$\\mathrm{Green~Channel~S/N}$',\n",
    "    Li        = r'$\\mathrm{[Li/Fe]~[dex]}$',\n",
    "    C         = r'$\\mathrm{[C/Fe]~[dex]}$',\n",
    "    O         = r'$\\mathrm{[O/Fe]~[dex]}$',\n",
    "    Na        = r'$\\mathrm{[Na/Fe]~[dex]}$',\n",
    "    Mg        = r'$\\mathrm{[Mg/Fe]~[dex]}$',\n",
    "    Al        = r'$\\mathrm{[Al/Fe]~[dex]}$',\n",
    "    Si        = r'$\\mathrm{[Si/Fe]~[dex]}$',\n",
    "    K         = r'$\\mathrm{[K/Fe]~[dex]}$',\n",
    "    Ca        = r'$\\mathrm{[Ca/Fe]~[dex]}$',\n",
    "    Sc        = r'$\\mathrm{[Sc/Fe]~[dex]}$',\n",
    "    Ti        = r'$\\mathrm{[Ti/Fe]~[dex]}$',\n",
    "    V         = r'$\\mathrm{[V/Fe]~[dex]}$',\n",
    "    Cr        = r'$\\mathrm{[Cr/Fe]~[dex]}$',\n",
    "    Mn        = r'$\\mathrm{[Mn/Fe]~[dex]}$',\n",
    "    Co        = r'$\\mathrm{[Co/Fe]~[dex]}$',\n",
    "    Ni        = r'$\\mathrm{[Ni/Fe]~[dex]}$',\n",
    "    Cu        = r'$\\mathrm{[Cu/Fe]~[dex]}$',\n",
    "    Zn        = r'$\\mathrm{[Zn/Fe]~[dex]}$',\n",
    "    Rb        = r'$\\mathrm{[Rb/Fe]~[dex]}$',\n",
    "    Sr        = r'$\\mathrm{[Sr/Fe]~[dex]}$',\n",
    "    Y         = r'$\\mathrm{[Y/Fe]~[dex]}$',\n",
    "    Zr        = r'$\\mathrm{[Zr/Fe]~[dex]}$',\n",
    "    Mo        = r'$\\mathrm{[Mo/Fe]~[dex]}$',\n",
    "    Ru        = r'$\\mathrm{[Ru/Fe]~[dex]}$',\n",
    "    Ba        = r'$\\mathrm{[Ba/Fe]~[dex]}$',\n",
    "    La        = r'$\\mathrm{[La/Fe]~[dex]}$',\n",
    "    Ce        = r'$\\mathrm{[Ce/Fe]~[dex]}$',\n",
    "    Nd        = r'$\\mathrm{[Nd/Fe]~[dex]}$',\n",
    "    Sm        = r'$\\mathrm{[Sm/Fe]~[dex]}$',\n",
    "    Eu        = r'$\\mathrm{[Eu/Fe]~[dex]}$'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are creating a trainingset for the Cannon in IPYNB not PY mode, using default values for output/DR/mode/\n",
      "Cannon3.8 dr5.3 Sp\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Change Work directory (if Sven's computer)\n",
    "try:\n",
    "    localFilePath = '/shared-storage/buder/svn-repos/trunk/GALAH/'\n",
    "    os.chdir(localFilePath)\n",
    "except:\n",
    "    try:\n",
    "        localFilePath = '/avatar/buder/trunk/GALAH/'\n",
    "        os.chdir(localFilePath)\n",
    "    except:\n",
    "        print('Could not change Path to '+localFilePath)\n",
    "\n",
    "if sys.argv[1] == '-f':\n",
    "    print('You are creating a trainingset for the Cannon in IPYNB not PY mode, using default values for output/DR/mode/')\n",
    "    output   = 'Cannon3.8'\n",
    "    DR       = 'dr5.3'\n",
    "    mode     = 'Sp'\n",
    "    print(output,DR,mode)\n",
    "else:\n",
    "    print('You are running the Cannon in PY mode')\n",
    "    output   = sys.argv[1]\n",
    "    DR       = sys.argv[2]\n",
    "    try:\n",
    "        mode = sys.argv[3]\n",
    "        print(output,DR,mode)\n",
    "    except:\n",
    "        # No obs_date chosen, i.e. do the actual training step\n",
    "        mode = 'Sp'\n",
    "        print(output,DR,mode)\n",
    "    \n",
    "# IRAF REDUCTION VERSION\n",
    "\n",
    "#DR                  = 'dr5.3'  # default: 'dr5.3', this code is also compatible with 'dr5.2','dr5.1'\n",
    "backup_DR_date      = '180227' # insert here only the last known date! By default, the code will try to use the latest\n",
    "complete_DR         = True     # default: True, otherwise provide files in 'SPECTRA/FIELD/*.fits'\n",
    "field               = ''       # default: not set, if complete_DR == False, set field name here\n",
    "\n",
    "# ADDITIONAL CORRECTIONS BY WG4\n",
    "\n",
    "telluric_correction = False #True\n",
    "skyline_correction  = True  #True\n",
    "renormalise         = False\n",
    "\n",
    "# CANNON SPECIFICATIONS\n",
    "\n",
    "include_ccd4        = True\n",
    "subset              = '_SMEmasks'\n",
    "mode_in             = '_'+mode\n",
    "filteroff           = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iteration_outlier = []\n",
    "\n",
    "iterations = glob.glob('CANNON/'+DR+'/'+output+'/'+output+mode_in+subset+'_model.pickle')\n",
    "\n",
    "print(iterations)\n",
    "\n",
    "if len(iterations) > 0:\n",
    "    print('STARTING NEW ITERATION!')\n",
    "    door=open('CANNON/'+DR+'/'+output+'/'+output+mode_in+subset+'_training_data.pickle')\n",
    "    training_spectra,filters,training_label,labels,training_fits,training_galah_ids=pickle.load(door)\n",
    "    door.close()\n",
    "    try:\n",
    "        door=open('CANNON/'+DR+'/'+output+'/'+output+mode_in+subset+'_selftest_tags.pickle')\n",
    "        label_self, error_label_self,  covs_self, chi2_self, ids_self, chi2_good_self, chi2_each_self=pickle.load(door)\n",
    "    except:\n",
    "        door=open('CANNON/'+DR+'/'+output+'/'+output+mode_in+subset+'_selftest_tags.pickle')\n",
    "        label_self, error_label_self,  covs_self, chi2_self, ids_self, chi2_good_self=pickle.load(door)\n",
    "    door.close()\n",
    "    \n",
    "if len(iterations) > 0:\n",
    "    \n",
    "    if len(labels) == 6:\n",
    "        f = plt.figure(figsize = (2*6.4, 2*4.8))\n",
    "    elif len(labels) == 7:\n",
    "        f = plt.figure(figsize = (8./3.*6.4, 3*4.8))\n",
    "    for each in range(len(label_self[0])):\n",
    "        bias = np.mean(training_label[:,each]-label_self[:,each])\n",
    "        std  = np.std(training_label[:,each]-label_self[:,each])\n",
    "        rms  = (np.sum([(xx-yy)**2 for xx,yy in zip(training_label[:,each],label_self[:,each])])/len(training_label[:,each]))**0.5\n",
    "        sigma_outlier   = np.where(abs(training_label[:,each]-label_self[:,each]) > 2. * rms)[0]\n",
    "\n",
    "        iteration_outlier.append(sigma_outlier)\n",
    "        \n",
    "        def ab_scatter(X, Y, ax=plt.gca, **kwargs):\n",
    "            \"\"\"\n",
    "            This function gives back a scatter plot\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            c = kwargs.get('c',parula_zero)\n",
    "            s = kwargs.get('s',2)\n",
    "            s1 = ax.scatter(X,Y,c=c,s=s,alpha=0.5,rasterized=True,label=r'$\\Delta$ iteration '+str(len(iterations)))\n",
    "\n",
    "            return ax\n",
    "\n",
    "        def ab_dens2d(X, Y, ax=plt.gca, min_per_bin=5, zeroslines=True, interimlines=True, colorbar=True, **kwargs):\n",
    "            \"\"\"\n",
    "            This function gives back a 2D density plot \n",
    "            of the data put in as X and Y with \n",
    "            all points as scatter below certain density\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            #first make sure to only use finite X and Y values\n",
    "            XY_finite = (np.isfinite(X) & np.isfinite(Y))\n",
    "            X = X[XY_finite]\n",
    "            Y = Y[XY_finite]\n",
    "\n",
    "            # General kwargs:\n",
    "            xlabel = kwargs.get('xlabel',tex_dict[labels[each]]+' input-output')\n",
    "            ylabel = kwargs.get('ylabel', r'$\\Delta$~'+tex_dict[labels[each]]+' input-output')\n",
    "            xlim   = kwargs.get('xlim', (-3.0,0.65))\n",
    "            ylim   = kwargs.get('ylim', (-0.5,1.00))\n",
    "            cmap = kwargs.get('cmap', parula)\n",
    "            bins = kwargs.get('bins', (0.05,0.025))\n",
    "            if np.shape(bins) != ():\n",
    "                # assuming input in dex\n",
    "                bins = bins\n",
    "\n",
    "            # plot all points as scatter before density structure is overlaid\n",
    "            scatter = ab_scatter(X,Y,ax=ax)\n",
    "\n",
    "            H, xedges, yedges = np.histogram2d(X,Y,bins=bins)\n",
    "            H=np.rot90(H)\n",
    "            H=np.flipud(H)\n",
    "            Hmasked = np.ma.masked_where(H<min_per_bin,H)\n",
    "\n",
    "            dens2d=ax.pcolormesh(xedges,yedges,Hmasked,cmap=cmap)\n",
    "\n",
    "            ax.set_xlabel(xlabel)\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.set_xlim(xlim)\n",
    "            ax.set_ylim(ylim)\n",
    "            xticks = kwargs.get('xticks',ax.get_xticks())\n",
    "            ax.set_xticks(xticks)\n",
    "\n",
    "            ax.axhline(0,c='k',lw=0.5)\n",
    "            \n",
    "            if colorbar == True:\n",
    "                c = plt.colorbar(dens2d,ax=ax)\n",
    "                c.set_label('Counts')\n",
    "\n",
    "        if len(labels) == 6:\n",
    "            ax = plt.subplot(3,2,each+1)\n",
    "        elif len(labels) == 7:\n",
    "            ax = plt.subplot(4,2,each+1)\n",
    "            \n",
    "        label_min = np.min([np.nanmin(training_label[:,each]),np.nanmin(label_self[:,each])])\n",
    "        label_max = np.max([np.nanmax(training_label[:,each]),np.nanmax(label_self[:,each])])\n",
    "        label_mima = label_max - label_min\n",
    "        \n",
    "        dlabel_min = np.nanmin(training_label[:,each]-label_self[:,each])\n",
    "        dlabel_max = np.nanmax(training_label[:,each]-label_self[:,each])\n",
    "        dlabel_dd  = np.max([abs(dlabel_min),abs(dlabel_max)])\n",
    "        \n",
    "        ab_dens2d(\n",
    "            ax=ax,\n",
    "            X = training_label[:,each],\n",
    "            Y = training_label[:,each]-label_self[:,each],\n",
    "            xlim = (label_min-0.05*label_mima,label_max+0.05*label_mima),\n",
    "            ylim = (-1.05*dlabel_dd,1.05*dlabel_dd),\n",
    "            bins = [\n",
    "                np.arange(label_min-0.05*label_mima,label_max+0.05*label_mima+0.001,1.1*label_mima/40.),\n",
    "                np.arange(-1.05*dlabel_dd,1.05*dlabel_dd+0.001,2.1*dlabel_dd/40.)\n",
    "                ]\n",
    "            )\n",
    "        ax.text(0.025,0.95,'Bias: ',ha='left',va='center',transform=ax.transAxes)\n",
    "        ax.text(0.025,0.90,'Std:  ',ha='left',va='center',transform=ax.transAxes)\n",
    "        ax.text(0.025,0.85,'RMS:  ',ha='left',va='center',transform=ax.transAxes)\n",
    "        ax.text(0.175,0.95,str(0.01*round(100*bias)),ha='left',va='center',transform=ax.transAxes)\n",
    "        ax.text(0.175,0.90,str(0.01*round(100*std)),ha='left',va='center',transform=ax.transAxes)\n",
    "        ax.text(0.175,0.85,str(0.01*round(100*rms)),ha='left',va='center',transform=ax.transAxes)\n",
    "        si1 = ax.axhline(2.*rms,c='k',ls='dashed',label=r'$2\\,\\sigma~\\mathrm{outlier}$')\n",
    "        si2 = ax.axhline(-2.*rms,c='k',ls='dashed')\n",
    "        ax.legend(loc='lower right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('CANNON/'+DR+'/'+output+'/diagnostic_plots/'+output+mode_in+subset+'_it1_outlier.pdf',dpi=600)\n",
    "        #plt.close()\n",
    "        \n",
    "    iteration_outlier = np.unique(np.concatenate((iteration_outlier)))\n",
    "\n",
    "    t = table.Table()\n",
    "    ts_old = t.read('CANNON/'+DR+'/'+output+'/trainingset/'+output+mode_in+subset+'_trainingset.fits')\n",
    "    print('ORIGINAL FITS HAS '+str(len(ts_old['sobject_id']))+' ENTRIES')\n",
    "\n",
    "    subset = subset+'_it'+str(len(iterations))\n",
    "    print('THIS IS ITERATION NR. '+str(len(iterations)))\n",
    "    print('EXCLUDING '+str(len(iteration_outlier))+' SPECTRA ('+str(\"{:.1f}\".format((100*len(iteration_outlier))/len(training_label)))+'\\% OF '+str(len(training_label))+' SPECTRA)')\n",
    "\n",
    "    np.savetxt('CANNON/'+DR+'/'+output+'/trainingset/'+output+mode_in+subset+'_leftout',iteration_outlier,fmt='%s')\n",
    "\n",
    "    print(len(ts_old))\n",
    "    ts_old.remove_rows(iteration_outlier)\n",
    "\n",
    "    print(len(ts_old))\n",
    "\n",
    "    print(str(len(ts_old['sobject_id']))+' STARS LEFT')\n",
    "    ts_old.write('CANNON/'+DR+'/'+output+'/trainingset/'+output+mode_in+subset+'_trainingset.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if output == 'Cannon3.8':\n",
    "    include_parameters = np.array(['Teff','Logg','Fe_H','Vmic','Vsini','Alpha_Fe']) # 'M_H', Alpha_Fe',\n",
    "else:\n",
    "    include_parameters = np.array(['Teff','Logg','Fe_H','Vmic','Vsini']) # 'M_H', Alpha_Fe',\n",
    "include_auxiliary  = np.array(['Ak']) # ['Ebv']\n",
    "include_abundances = np.array([])\n",
    "if mode != 'Sp':\n",
    "    include_abundances = np.array([mode])\n",
    "#include_abundances = np.array(['O','Na','Mg','Si','Ca','Ti','Cr'])\n",
    "\n",
    "labels = np.concatenate((include_parameters,include_auxiliary,include_abundances))\n",
    "\n",
    "# Define the 4 CCD grids for the Cannon leaving at least 20 km/s to ab lines\n",
    "x1=np.arange(4715.94,4896.00,0.046) # ab lines 4716.3 - 4892.3\n",
    "x2=np.arange(5650.06,5868.25,0.055) # ab lines 5646.0 - 5867.8\n",
    "x3=np.arange(6480.52,6733.92,0.064) # ab lines 6481.6 - 6733.4\n",
    "x4=np.arange(7693.50,7875.55,0.074) # ab lines 7691.2 - 7838.5\n",
    "\n",
    "include_ccd4=True#False\n",
    "plot_spectra=False#True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "#       IMPORT OF SOBJECT IRAF         #\n",
    "########################################\n",
    "\n",
    "# This file was intended for 'dr52' but is also compatible with irafdr51\n",
    "\n",
    "if DR == 'dr5.3':\n",
    "    versions = glob.glob('DATA/sobject_iraf_53_180227.fits')\n",
    "elif DR == 'dr5.2':\n",
    "    versions = glob.glob('DATA/sobject_iraf_52_170926.fits')\n",
    "else:\n",
    "    versions = ['DATA/iraf_dr51_09232016_corrected.fits']\n",
    "\n",
    "# Read in information from the IRAF FITS\n",
    "door = pyfits.open(versions[-1])\n",
    "iraf = door[1].data\n",
    "door.close()\n",
    "\n",
    "print(versions[-1]+' will be used.')\n",
    "print('Available entries in IRAF FITS:  '+str(len(iraf['sobject_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Cannon version --- '+output+subset+' --- will be created')\n",
    "print('GUESS-normalized spectra FITS[4] will be used')\n",
    "\n",
    "# Initialize variables that will be filled later\n",
    "flux_take = []\n",
    "wavelx_take = []\n",
    "npix = len(x1)+len(x2)+len(x3)\n",
    "if include_ccd4 == True:\n",
    "        npix += len(x4)\n",
    "\n",
    "error_take = []\n",
    "name_take = [] \n",
    "galah_id_take = []\n",
    "usable = []\n",
    "\n",
    "# speed of light and definition of large errors\n",
    "clight = 299792.458 # speed of light in km/s\n",
    "large = 100.\n",
    "\n",
    "possible_parameters = np.array(['Teff','Logg','Fe_H','M_H','Alpha_Fe','Vmic','Vsini'])\n",
    "possible_auxiliary  = np.array(['Ak','Ebv'])\n",
    "possible_abundances = np.array(['Li','C','O','Na','Mg','Al','Al6696','Al7835','Si','K','K5802','K7699','Ca','Sc','Ti','V','Cr','Mn','Co','Ni','Cu','Cu5700','Cu5782','Zn','Rb','Sr','Y','Zr','Mo','Ru','Ba','Ba5854','Ba6497','La','Ce','Nd','Sm','Eu'])\n",
    "possible_labels     = np.concatenate((possible_parameters,possible_auxiliary,possible_abundances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in FITS-data and labels from each chosen field(s)\n",
    "t = table.Table()\n",
    "\n",
    "sme = t.read('CANNON/'+DR+'/'+output+'/trainingset/'+output+mode_in+subset+'_trainingset.fits')\n",
    "\n",
    "spectratotake=np.arange(len(sme['sobject_id']))\n",
    "\n",
    "name_fits=sme['sobject_id']\n",
    "galah_id=sme['galah_id']\n",
    "field_id=sme['field']\n",
    "vrad=sme['rv_sme']\n",
    "\n",
    "possible_metaall = np.ones((len(sme['Teff_sme']),),dtype={'names':possible_labels,'formats':[float for it in range(len(possible_labels))]})\n",
    "possible_filters = np.ones((npix,),dtype={'names':possible_labels,'formats':[int for it in range(len(possible_labels))]})\n",
    "\n",
    "for i in possible_parameters:\n",
    "        possible_metaall[i] = sme[i+'_sme']\n",
    "possible_metaall['Ebv'] = sme['ebv']\n",
    "possible_metaall['Ak'] = sme['Ak']\n",
    "\n",
    "# Filter for Alpha_Fe based on Mg, Si, Ca and Ti\n",
    "alpha_filters = np.zeros(npix)\n",
    "for i in ['Mg', 'Si', 'Ca', 'Ti']:\n",
    "    alpha_filters += np.array(np.loadtxt('CANNON/'+DR+'/masks_4ccds/DR2_'+i+'.txt',usecols=(5,),unpack=1))\n",
    "alpha_filters.clip(max=1)\n",
    "possible_filters['Alpha_Fe'] = alpha_filters\n",
    "\n",
    "for i in possible_abundances:\n",
    "    #if output != 'Cannon3.2':\n",
    "    #    print('Using '+i+'_H_sme')\n",
    "    #    possible_metaall[i] = sme[i+'_H_sme']\n",
    "    #else:\n",
    "    print('Using '+i+'_Fe_sme')\n",
    "    possible_metaall[i] = sme[i+'_Fe_sme']\n",
    "    \n",
    "if mode != 'Sp':    \n",
    "    if filteroff == 0:\n",
    "        if include_ccd4 == True:\n",
    "            possible_filters[mode] = np.loadtxt('CANNON/'+DR+'/masks_4ccds/DR2_'+mode+'.txt',usecols=(5,),unpack=1)\n",
    "        else:\n",
    "            possible_filters[mode] = np.loadtxt('CANNON/'+DR+'/masks_3ccds/DR2_'+mode+'.txt',usecols=(5,),unpack=1)                        \n",
    "    else:\n",
    "        sys.exit(str(i)+' not in sme / not used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snr = np.array(sme['snr2_c2_iraf'])\n",
    "\n",
    "print('DONE reading in labels: '+str(len(name_fits))+' labels available')\n",
    "\n",
    "snr_cut1=np.min(snr)\n",
    "snr_cut2=np.max(snr)\n",
    "\n",
    "low=[]\n",
    "high=[]\n",
    "high_fits = []\n",
    "notfound=[]\n",
    "\n",
    "for each in spectratotake:\n",
    "        try:\n",
    "            #print(each,name_fits[each])\n",
    "            if DR == 'dr5.3':\n",
    "                try:\n",
    "                    fits1 = pyfits.open(\"SPECTRA/dr5.3/\"+str(name_fits[each])[0:6]+\"/standard/com/\"+str(name_fits[each])+\"1.fits\")\n",
    "                except:\n",
    "                    try:\n",
    "                        fits1 = pyfits.open(\"SPECTRA/dr5.2/\"+str(name_fits[each])[0:6]+\"/standard/com/\"+str(name_fits[each])+\"1.fits\")\n",
    "                    except:\n",
    "                        sys.exit()\n",
    "            elif DR == 'dr5.2':\n",
    "                try:\n",
    "                    fits1 = pyfits.open(\"SPECTRA/dr5.2/\"+str(name_fits[each])[0:6]+\"/standard/com/\"+str(name_fits[each])+\"1.fits\")\n",
    "                except:\n",
    "                    fits1 = pyfits.open(\"SPECTRA/irafdr51/\"+str(name_fits[each])[0:6]+\"/combined/\"+str(name_fits[each])+\"1.fits\")\n",
    "            else:\n",
    "                fits1 = pyfits.open(\"SPECTRA/irafdr51/\"+str(name_fits[each])[0:6]+\"/combined/\"+str(name_fits[each])+\"1.fits\")\n",
    "            if fits1[0].header['SLITMASK']=='OUT':\n",
    "                low.append(each)\n",
    "            else:\n",
    "                high.append(each)\n",
    "                high_fits.append(name_fits[each])\n",
    "        except:\n",
    "                notfound.append(name_fits[each])\n",
    "        fits1.close()\n",
    "\n",
    "        \n",
    "spectratotake=np.array(low)\n",
    "\n",
    "keep=[]\n",
    "for i in range(0,len(spectratotake)):\n",
    "    if name_fits[spectratotake[i]] not in iteration_outlier:\n",
    "        keep.append(spectratotake[i])\n",
    "spectratotake=np.array(keep)\n",
    "print(str(len(spectratotake))+' will be used for trainingset')\n",
    "print('Taken out:')\n",
    "print('HighRes spectra:   '+str(len(high)))\n",
    "print(list(high_fits))\n",
    "print('Not found spectra: '+str(len(notfound)))\n",
    "print('Iteration outlier: '+str(len(iteration_outlier)))\n",
    "\n",
    "np.savetxt('CANNON/'+DR+'/'+output+'/element_runs/'+mode+subset+'_used.txt',np.array(spectratotake),fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for each in spectratotake:\n",
    "    try:\n",
    "        \n",
    "        error = 0\n",
    "        \n",
    "        each_fits = str(name_fits[each])\n",
    "\n",
    "        if np.where(spectratotake==each)[0]%100==0: \n",
    "            print(str(0.01*round(np.where(spectratotake==each)[0]*10000.00/len(spectratotake)))+' %')\n",
    "\n",
    "        # Only necessary, if VRAD/VBARY needed\n",
    "\n",
    "        if (telluric_correction == True) | (skyline_correction  == True):\n",
    "\n",
    "            # Cross-match FITS name with IRAF SOBJECT_ID\n",
    "            try:\n",
    "                fits_in_iraf = np.where(int(each_fits) == iraf['sobject_id'])[0]\n",
    "                fits_in_iraf = fits_in_iraf[0]\n",
    "            except:\n",
    "                sys.exit('The FITS '+each_fits+' is not in IRAF '+DR)\n",
    "\n",
    "            # Pull VRAD and V_BARY from IRAF or other source\n",
    "\n",
    "            if DR in ['dr5.2', 'dr5.3']:\n",
    "                vrad = iraf['rv_guess_shift'][fits_in_iraf]\n",
    "                v_bary = iraf['v_bary'][fits_in_iraf]\n",
    "                #print(iraf['red_flag'][fits_in_iraf])\n",
    "            if DR == 'dr51':\n",
    "                vrad = iraf['vrad'][fits_in_iraf]\n",
    "                combs=len(fits1[0].header['COMB*'])\n",
    "                bary_fits=pyfits.open('DATA/GALAH_vbary_09232016.fits')\n",
    "                v_bary=[]\n",
    "                for baries in range(0,combs):\n",
    "                    bary_pos=np.where(bary_fits[1].data['out_name']==fits1[0].header['COMB'+str(baries)])[0]\n",
    "                    if len(bary_pos)==1:\n",
    "                            v_bary.append(bary_fits[1].data['v_bary'][bary_pos[0]])\n",
    "                    else:\n",
    "                            sys.exit('NO V_BARY ENTRY FOUND')\n",
    "                v_bary=np.mean(v_bary)\n",
    " \n",
    "        ''' IMPORT CCD1 '''\n",
    "\n",
    "        error += 1 # if error == 1: no fits 1\n",
    "        \n",
    "        if complete_DR == True:\n",
    "            if DR=='dr5.3':\n",
    "                try:\n",
    "                    fits1 = pyfits.open(\"SPECTRA/dr5.3/\"+each_fits[0:6]+\"/standard/com/\"+each_fits+\"1.fits\")\n",
    "                except:\n",
    "                    fits1 = pyfits.open(\"SPECTRA/dr5.2/\"+each_fits[0:6]+\"/standard/com/\"+each_fits+\"1.fits\")\n",
    "            if DR=='dr5.2':\n",
    "                try:\n",
    "                    fits1 = pyfits.open(\"SPECTRA/dr5.2/\"+each_fits[0:6]+\"/standard/com/\"+each_fits+\"1.fits\")\n",
    "                except:\n",
    "                    fits1 = pyfits.open(\"SPECTRA/irafdr51/\"+each_fits[0:6]+\"/combined/\"+each_fits+\"1.fits\")\n",
    "            if DR=='dr5.1':\n",
    "                    fits1 = pyfits.open(\"SPECTRA/irafdr51/\"+each_fits[0:6]+\"/combined/\"+each_fits+\"1.fits\")\n",
    "        else:\n",
    "            fits1 = pyfits.open(\"SPECTRA/\"+field+\"/\"+each_fits+\"1.fits\")\n",
    "\n",
    "        if telluric_correction == True:\n",
    "            telluric_fits = pyfits.open('DATA/telluric_noao_21k.fits')\n",
    "            wave_tel      = telluric_fits[1].data['wave']/(1.0+(vrad-v_bary)/clight)\n",
    "\n",
    "        if skyline_correction  == True:\n",
    "            sky_mask=pyfits.open('DATA/Skyspectrum_161105.fits')\n",
    "            wave_sky=sky_mask[1].data['wave']/(1.0+(vrad-v_bary)/clight)\n",
    "\n",
    "        error += 1 # if error == 2: no fits extension\n",
    "        ws=fits1[4].header[\"CRVAL1\"]\n",
    "        inc=fits1[4].header[\"CDELT1\"]\n",
    "        nax=fits1[4].header[\"NAXIS1\"]\n",
    "        ref=fits1[4].header[\"CRPIX1\"]\n",
    "        if ref == 0:\n",
    "            ref=1\n",
    "        x1raw=map(lambda x:((x-ref+1)*inc+ws),range(0,nax))\n",
    "\n",
    "        error += 1 # if error == 3: something with extension 4 of ccd1\n",
    "\n",
    "        # save normalized flux to y1 and uncertainties to z1\n",
    "        if renormalise!=True:\n",
    "            # EITHER TAKE FITS-EXTENSION 4: NORMALIZED FLUX\n",
    "            y1raw=fits1[4].data[0:nax]\n",
    "            z1raw=fits1[4].data[0:nax]*fits1[1].data[0:nax]\n",
    "            y1=np.interp(x1,x1raw,y1raw)\n",
    "            z1=np.interp(x1,x1raw,z1raw)\n",
    "                        #z1=y1/np.sqrt(np.median(fits1[0].data)*fits[0].header[\"RO_GAIN\"])\n",
    "        else:\n",
    "            # OR USE FITS-EXTENSION 0: REDUCED FLUX AND RENORMALIZE\n",
    "            y1raw=fits1[4].data[0:nax]\n",
    "            z1raw=fits1[4].data[0:nax]*fits1[1].data[0:nax]\n",
    "            y1=np.interp(x1,x1raw,y1raw)\n",
    "            z1=np.interp(x1,x1raw,z1raw)\n",
    "            #fit chebychev 2nd order polynomial to fits-extension 0 with continuum pixels estimated during prior training step\n",
    "            fit1 = np.polynomial.chebyshev.Chebyshev.fit(x=x1[cont1], y=y1[cont1], w=z1[cont1] , deg=3)\n",
    "            y1=y1/fit1(x1)\n",
    "            z1=y1/np.sqrt(np.median(fits1[0].data)*fits1[0].header[\"RO_GAIN\"])\n",
    "\n",
    "        if telluric_correction == True:\n",
    "            telluric_interp=np.interp(x1,wave_tel,telluric_fits[1].data['flux'])\n",
    "            telluric_interp[np.logical_or(np.isnan(telluric_interp),telluric_interp<0.81)]=0.81\n",
    "            telluric_interp[telluric_interp>0.995]=1.0\n",
    "            z1 += (1./(telluric_interp*5.-4) - 1.)\n",
    "\n",
    "        if skyline_correction == True:\n",
    "            sky_interp=np.interp(x1,wave_sky,sky_mask[1].data['sky'])\n",
    "            z1 += large*sky_interp\n",
    "\n",
    "        y1[np.logical_or(x1<=x1raw[0],x1>=x1raw[-1])]=1.\n",
    "        z1[np.logical_or(x1<=x1raw[0],x1>=x1raw[-1])]=large\n",
    "        fits1.close()\n",
    "\n",
    "        error += 1 # if error == 4: something with extension 4 of ccd2\n",
    "\n",
    "        ''' IMPORT CCD2 '''\n",
    "        if complete_DR == True:\n",
    "            if DR=='dr5.3':\n",
    "                try:\n",
    "                    fits2 = pyfits.open(\"SPECTRA/dr5.3/\"+each_fits[0:6]+\"/standard/com/\"+each_fits+\"2.fits\")\n",
    "                except:\n",
    "                    fits2 = pyfits.open(\"SPECTRA/dr5.2/\"+each_fits[0:6]+\"/standard/com/\"+each_fits+\"2.fits\")\n",
    "            if DR=='dr5.2':\n",
    "                try:\n",
    "                    fits2 = pyfits.open(\"SPECTRA/dr5.2/\"+each_fits[0:6]+\"/standard/com/\"+each_fits+\"2.fits\")\n",
    "                except:\n",
    "                    fits2 = pyfits.open(\"SPECTRA/irafdr51/\"+each_fits[0:6]+\"/combined/\"+each_fits+\"2.fits\")\n",
    "            if DR=='dr5.1':\n",
    "                    fits2 = pyfits.open(\"SPECTRA/irafdr51/\"+each_fits[0:6]+\"/combined/\"+each_fits+\"2.fits\")\n",
    "        else:\n",
    "            fits2 = pyfits.open(\"SPECTRA/\"+field+\"/\"+each_fits+\"2.fits\")\n",
    "\n",
    "        ws=fits2[4].header[\"CRVAL1\"]\n",
    "        inc=fits2[4].header[\"CDELT1\"]\n",
    "        nax=fits2[4].header[\"NAXIS1\"]\n",
    "        ref=fits2[4].header[\"CRPIX1\"]\n",
    "        if ref == 0:\n",
    "            ref=1\n",
    "        x2raw=map(lambda x:((x-ref+1)*inc+ws),range(0,nax))\n",
    "                # save normalized flux to y2 and uncertainties to z2\n",
    "        if renormalise!=True:\n",
    "            # EITHER TAKE FITS-EXTENSION 4: NORMALIZED FLUX\n",
    "            y2raw=fits2[4].data[0:nax]\n",
    "            z2raw=fits2[4].data[0:nax]*fits2[1].data[0:nax]\n",
    "            y2=np.interp(x2,x2raw,y2raw)\n",
    "            z2=np.interp(x2,x2raw,z2raw)\n",
    "            #z2=y2/np.sqrt(np.median(fits2[0].data)*fits2[0].header[\"RO_GAIN\"])\n",
    "        else:\n",
    "            # OR USE FITS-EXTENSION 4: REDUCED FLUX AND RENORMALIZE\n",
    "            y2raw=fits2[4].data[0:nax]\n",
    "            z2raw=fits2[4].data[0:nax]*fits2[1].data[0:nax]\n",
    "            y2=np.interp(x2,x2raw,y2raw)\n",
    "            z2=np.interp(x2,x2raw,z2raw)\n",
    "            #fit chebychev 2nd order polynomial to fits-extension 0 with continuum pixels estimated during prior training step\n",
    "            fit2 = np.polynomial.chebyshev.Chebyshev.fit(x=x2[cont2], y=y2[cont2], w=z2[cont2] , deg=2) # there could be weights included, but since we assume same S/N for GALAH, this would not be helpful\n",
    "            y2=y2/fit2(x2)\n",
    "            z2=y2/np.sqrt(np.median(fits2[0].data)*fits2[0].header[\"RO_GAIN\"])\n",
    "\n",
    "        if telluric_correction == True:\n",
    "            telluric_interp=np.interp(x2,wave_tel,telluric_fits[1].data['flux'])\n",
    "            telluric_interp[np.logical_or(np.isnan(telluric_interp),telluric_interp<0.81)]=0.81\n",
    "            telluric_interp[telluric_interp>0.995]=1.0\n",
    "            z2 += (1./(telluric_interp*5.-4)-1.)\n",
    "\n",
    "        if skyline_correction == True:\n",
    "            sky_interp=np.interp(x2,wave_sky,sky_mask[1].data['sky'])\n",
    "            z2 += large*sky_interp\n",
    "\n",
    "        y2[np.logical_or(x2<=x2raw[0],x2>=x2raw[-1])]=1.\n",
    "        z2[np.logical_or(x2<=x2raw[0],x2>=x2raw[-1])]=large\n",
    "        fits2.close()\n",
    "\n",
    "        error += 1 # if error == 5: something with extension 4 of ccd3\n",
    "\n",
    "        ''' IMPORT CCD3 '''\n",
    "        if complete_DR == True:\n",
    "            if DR=='dr5.3':\n",
    "                try:\n",
    "                    fits3 = pyfits.open(\"SPECTRA/dr5.3/\"+each_fits[0:6]+\"/standard/com/\"+each_fits+\"3.fits\")\n",
    "                except:\n",
    "                    fits3 = pyfits.open(\"SPECTRA/dr5.2/\"+each_fits[0:6]+\"/standard/com/\"+each_fits+\"3.fits\")\n",
    "            if DR=='dr5.2':\n",
    "                try:\n",
    "                    fits3 = pyfits.open(\"SPECTRA/dr5.2/\"+each_fits[0:6]+\"/standard/com/\"+each_fits+\"3.fits\")\n",
    "                except:\n",
    "                    fits3 = pyfits.open(\"SPECTRA/irafdr51/\"+each_fits[0:6]+\"/combined/\"+each_fits+\"3.fits\")\n",
    "            if DR=='dr5.1':\n",
    "                    fits3 = pyfits.open(\"SPECTRA/irafdr51/\"+each_fits[0:6]+\"/combined/\"+each_fits+\"3.fits\")\n",
    "        else:\n",
    "            fits3 = pyfits.open(\"SPECTRA/\"+field+\"/\"+each_fits+\"3.fits\")\n",
    "\n",
    "        ws=fits3[4].header[\"CRVAL1\"]\n",
    "        inc=fits3[4].header[\"CDELT1\"]\n",
    "        nax=fits3[4].header[\"NAXIS1\"] # taken fixed 4096 because of varying nax +-2\n",
    "        #nax=4096\n",
    "        ref=fits3[4].header[\"CRPIX1\"]\n",
    "        if ref == 0:\n",
    "            ref=1\n",
    "        x3raw=map(lambda x:((x-ref+1)*inc+ws),range(0,nax))\n",
    "        # save normalized flux to y3 and uncertainties to z3\n",
    "        if renormalise!=True:\n",
    "            # EITHER TAKE FITS-EXTENSION 4: NORMALIZED FLUX\n",
    "            y3raw=fits3[4].data[0:nax]\n",
    "            z3raw=fits3[4].data[0:nax]*fits3[1].data[0:nax]\n",
    "            y3=np.interp(x3,x3raw,y3raw)\n",
    "            z3=np.interp(x3,x3raw,z3raw)\n",
    "            #z3=y3/np.sqrt(np.median(fits3[0].data)*fits3[0].header[\"RO_GAIN\"])\n",
    "        else:\n",
    "            # OR USE FITS-EXTENSION 4: REDUCED FLUX AND RENORMALIZE\n",
    "            y3raw=fits3[4].data[0:nax]\n",
    "            y3=np.interp(x3,x3raw,y3raw)\n",
    "            #fit chebychev 2nd order polynomial to fits-extension 0 with continuum pixels estimated during prior training step\n",
    "            fit3 = np.polynomial.chebyshev.Chebyshev.fit(x=x3[pixlist3], y=y3[pixlist3] , deg=2) # there could be weights included, but since we assume same S/N for GALAH, this would not be helpful\n",
    "            y3=y3/fit3(x3)\n",
    "            z3=y3/np.sqrt(np.median(fits3[0].data)*fits3[0].header[\"RO_GAIN\"])\n",
    "\n",
    "        if telluric_correction == True:\n",
    "            telluric_interp=np.interp(x3,wave_tel,telluric_fits[1].data['flux'])\n",
    "            telluric_interp[np.logical_or(np.isnan(telluric_interp),telluric_interp<0.81)]=0.81\n",
    "            telluric_interp[telluric_interp>0.995]=1.0\n",
    "            z3 += (1./(telluric_interp*5.-4) - 1.)\n",
    "\n",
    "        if skyline_correction == True:\n",
    "            sky_interp=np.interp(x3,wave_sky,sky_mask[1].data['sky'])\n",
    "            z3 += large*sky_interp\n",
    "\n",
    "        y3[np.logical_or(x3<=x3raw[0],x3>=x3raw[-1])]=1.\n",
    "        z3[np.logical_or(x3<=x3raw[0],x3>=x3raw[-1])]=large\n",
    "        fits3.close()\n",
    "\n",
    "        error += 1 # if error == 6: something with extension 4 of ccd4\n",
    "\n",
    "        ''' IMPORT CCD4 '''\n",
    "        if include_ccd4 == True:\n",
    "            if complete_DR == True:\n",
    "                if DR=='dr5.3':\n",
    "                    try:\n",
    "                        fits4 = pyfits.open(\"SPECTRA/dr5.3/\"+each_fits[0:6]+\"/standard/com/\"+each_fits+\"4.fits\")\n",
    "                    except:\n",
    "                        fits4 = pyfits.open(\"SPECTRA/dr5.2/\"+each_fits[0:6]+\"/standard/com/\"+each_fits+\"4.fits\")\n",
    "                if DR=='dr5.2':\n",
    "                    try:\n",
    "                        fits4 = pyfits.open(\"SPECTRA/dr5.2/\"+each_fits[0:6]+\"/standard/com/\"+each_fits+\"4.fits\")\n",
    "                    except:\n",
    "                        fits4 = pyfits.open(\"SPECTRA/irafdr51/\"+each_fits[0:6]+\"/combined/\"+each_fits+\"4.fits\")\n",
    "                if DR=='dr5.1':\n",
    "                        fits4 = pyfits.open(\"SPECTRA/irafdr51/\"+each_fits[0:6]+\"/combined/\"+each_fits+\"4.fits\")\n",
    "            else:\n",
    "                fits4 = pyfits.open(\"SPECTRA/\"+field+\"/\"+each_fits+\"4.fits\")\n",
    "\n",
    "            ws=fits4[4].header[\"CRVAL1\"]\n",
    "            inc=fits4[4].header[\"CDELT1\"]\n",
    "            nax=fits4[4].header[\"NAXIS1\"] # taken fixed 4096 because of varying nax +-2\n",
    "            naxir=fits4[1].header[\"NAXIS1\"]\n",
    "            #nax=4096\n",
    "            ref=fits4[4].header[\"CRPIX1\"]\n",
    "            if ref == 0:\n",
    "                ref=1\n",
    "            # ir_cut is included, because of the low wavelength cut in fits-extension 4 (to get rid of H20 band < 7700)\n",
    "            #ir_cut=len(fits4[4].data)\n",
    "            x4raw=map(lambda x:((x-ref+1)*inc+ws),range(0,nax))\n",
    "\n",
    "            # save normalized flux to y4 and uncertainties to z4\n",
    "            if renormalise!=True:\n",
    "                # EITHER TAKE FITS-EXTENSION 4: NORMALIZED FLUX\n",
    "                y4raw=fits4[4].data[0:nax]\n",
    "                z4raw=fits4[4].data[0:nax]*fits4[1].data[naxir-nax:naxir]\n",
    "                y4=np.interp(x4,x4raw,y4raw)\n",
    "                z4=np.interp(x4,x4raw,z4raw)\n",
    "                #z4=y4/np.sqrt(np.median(fits4[0].data)*fits4[0].header[\"RO_GAIN\"])\n",
    "            else:\n",
    "                # OR USE FITS-EXTENSION 4: REDUCED FLUX AND RENORMALIZE\n",
    "                y4raw=fits4[4].data[nax-ir_cut:nax]\n",
    "                y4s=np.interp(x4,x4raw,y4raw)\n",
    "                #fit chebychev 2nd order polynomial to fits-extension 0 with continuum pixels estimated during prior training step\n",
    "                fit4 = np.polynomial.chebyshev.Chebyshev.fit(x=x4[pixlist4], y=y4s[pixlist4] , deg=4) # there could be weights included, but since we assume same S/N for GALAH, this would not be helpful\n",
    "                y4=y4s/fit4(x4)\n",
    "                z4=y4/np.sqrt(np.median(fits4[0].data)*fits4[0].header[\"RO_GAIN\"])\n",
    "\n",
    "            if telluric_correction == True:\n",
    "                telluric_interp=np.interp(x4,wave_tel,telluric_fits[1].data['flux'])\n",
    "                telluric_interp[np.logical_or(np.isnan(telluric_interp),telluric_interp<0.81)]=0.81\n",
    "                telluric_interp[telluric_interp>0.995]=1.0\n",
    "                z4 += (1./(telluric_interp*5.-4) -1.)\n",
    "\n",
    "            if skyline_correction == True:\n",
    "                sky_interp=np.interp(x4,wave_sky,sky_mask[1].data['sky'])\n",
    "                z4 += large*sky_interp\n",
    "\n",
    "            y4[np.logical_or(x4<=x4raw[0],x4>=x4raw[-1])]=1.\n",
    "            z4[np.logical_or(x4<=x4raw[0],x4>=x4raw[-1])]=large\n",
    "            fits4.close()\n",
    "            \n",
    "        error += 1 # if error == 7: something with combining the data\n",
    "\n",
    "        ''' COMBINE CCDs '''\n",
    "        if include_ccd4==True:\n",
    "            x = np.concatenate((x1,x2,x3,x4))\n",
    "            y = np.concatenate((y1,y2,y3,y4))\n",
    "            z = np.concatenate((z1,z2,z3,z4))\n",
    "        else:\n",
    "            x = np.concatenate((x1,x2,x3))\n",
    "            y = np.concatenate((y1,y2,y3))\n",
    "            z = np.concatenate((z1,z2,z3))\n",
    "\n",
    "        bady = np.isnan(y)\n",
    "        badz = np.isnan(z)\n",
    "        y[bady] = 1.\n",
    "        z[badz] = large\n",
    "        bady = np.logical_or(y > 1.2,y <0.0)\n",
    "        y[bady] = 1.\n",
    "        z[bady] = large\n",
    "\n",
    "        wavelx_take.append(x)\n",
    "        flux_take.append(y)\n",
    "        error_take.append(z)\n",
    "\n",
    "        name_take.append(name_fits[each])\n",
    "        galah_id_take.append(galah_id[each])\n",
    "\n",
    "        usable.append(each)\n",
    "        \n",
    "    except:\n",
    "        if error == 1:\n",
    "            print(each,name_fits[each],' from field '+str(field_id[each])+' error: 1st extension')\n",
    "        if error == 2:\n",
    "            print(each,name_fits[each],' from field '+str(field_id[each])+' error: 4th extenstion (GUESS flag?)')\n",
    "        if error == 3:\n",
    "            print(each,name_fits[each],' from field '+str(field_id[each])+' error: CCD1')\n",
    "        if error == 4:\n",
    "            print(each,name_fits[each],' from field '+str(field_id[each])+' error: CCD2')\n",
    "        if error == 5:\n",
    "            print(each,name_fits[each],' from field '+str(field_id[each])+' error: CCD3')\n",
    "        if error == 6:\n",
    "            print(each,name_fits[each],' from field '+str(field_id[each])+' error: CCD4')\n",
    "        if error == 7:\n",
    "            print(each,name_fits[each],' from field '+str(field_id[each])+' error: can not combine')\n",
    "\n",
    "print('DONE reading in spectra')\n",
    "\n",
    "# this combines the data into a single array of these vectors\n",
    "error_take = np.array(error_take)\n",
    "flux_take = np.array(flux_take)\n",
    "name_take = np.array(name_take)\n",
    "\n",
    "# we have three labels that we will train and solve for \n",
    "nmeta = len(labels) \n",
    "\n",
    "# the dataall is the actual spectral data \n",
    "# the metaall is the labels for the stasr of teff, logg and [fe/H]\n",
    "dataall = np.zeros((npix, len(name_take), 2))\n",
    "filterall = np.ones((npix, nmeta))\n",
    "metaall = np.ones((len(name_take), nmeta))\n",
    "countit = np.arange(0,len(flux_take),1)\n",
    "newwl = np.arange(0,len(flux_take),1) \n",
    "\n",
    "# populate the dataall array with the wavelength, flux and error\n",
    "for a,b,jj in zip(flux_take, error_take, countit):\n",
    "    dataall[:,jj,0] = a\n",
    "    dataall[:,jj,1] = b\n",
    "    \n",
    "nstars = np.shape(dataall)[1]\n",
    "\n",
    "#  check for bad pixels and set these to 1 and their uncertainties to the value \"large\"\n",
    "\n",
    "for jj in range(nstars):\n",
    "    bad = np.logical_or(dataall[:,jj,0] < 0.0, dataall[:,jj,0] > 1.2)\n",
    "    dataall[bad,jj,0] = 1.\n",
    "    dataall[bad,jj,1]  = large\n",
    "    \n",
    "# now we want to populate the metaall array with the labels \n",
    "for k in range(0,nmeta):\n",
    "        metaall[:,k]   = possible_metaall[labels[k]][np.array(usable)]\n",
    "        filterall[:,k] = possible_filters[labels[k]]\n",
    "\n",
    "# This is the name of the array that we are going to save into \n",
    "file_in = open('CANNON/'+DR+'/'+output+'/'+output+mode_in+subset+'_training_data.pickle', 'w')\n",
    "galah_id = np.array(galah_id) \n",
    "nstars = np.shape(dataall)[0]\n",
    "meds = np.median(dataall[:,:,0], axis =0) \n",
    "meds = np.array(meds)\n",
    "take1 = np.logical_and(meds > 0.8, meds < 1.1) # Upper limit was 1.1 before!\n",
    "\n",
    "print('shapes of pickle-output: ', np.shape(dataall[:,take1,:]),np.shape(filterall),np.shape(metaall[take1,:]),np.shape(labels),np.shape(name_take),np.shape(galah_id_take))\n",
    "pickle.dump((dataall[:,take1,:], filterall, metaall[take1,:], labels, name_take, galah_id_take),  file_in)\n",
    "file_in.close() \n",
    "\n",
    "print('DONE saving trainingset in *pickle*')\n",
    "print('Used labels :', end=' ')\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert IPYNB to PY\n",
    "\n",
    "os.chdir(localFilePath+'TheGALAHCannon/')\n",
    "\n",
    "convert_command = 'ipython nbconvert --to script Cannon_maketraining.ipynb'\n",
    "os.system(convert_command)\n",
    "\n",
    "os.chdir(localFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
