{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cannon TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "from scipy import optimize as opt\n",
    "import pickle \n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust OUTPUT/DR/OBS_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if sys.argv[1] == '-f':\n",
    "    print('You are running the Cannon in IPYNB not PY mode, using default values for output/DR/obs_date')\n",
    "    output   = 'Cannon3.0.1'\n",
    "    DR       = 'dr5.2'\n",
    "    mode     = 'O'\n",
    "    obs_date = ''\n",
    "    print(output,DR,mode,obs_date)\n",
    "else:\n",
    "    print('You are running the Cannon in PY mode')\n",
    "    output   = sys.argv[1]\n",
    "    DR       = sys.argv[2]\n",
    "    mode     = sys.argv[3]\n",
    "    try:\n",
    "        obs_date = sys.argv[4]\n",
    "        print(output,DR,mode,obs_date)\n",
    "    except:\n",
    "        # No obs_date chosen, i.e. do the actual training step\n",
    "        obs_date = ''\n",
    "        print(output,DR,mode,obs_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ccds     = '4'\n",
    "subset   = '_SMEmasks'\n",
    "\n",
    "mode_in = '_'+mode\n",
    "\n",
    "LARGE  = 2000.\n",
    "\n",
    "os.chdir('/shared-storage/buder/svn-repos/trunk/GALAH/')\n",
    "\n",
    "check_iterations = glob.glob('CANNON/'+DR+'/'+output+'/'+output+mode_in+subset+'_it*_training_data.pickle')\n",
    "print(check_iterations)\n",
    "if len(check_iterations) > 0:\n",
    "    subset = '_SMEmasks_it'+str(len(check_iterations))\n",
    "\n",
    "print(subset)\n",
    "\n",
    "LARGE  = 2000.\n",
    "\n",
    "trainingset  = 'CANNON/'+DR+'/'+output+'/'+output+mode_in+subset+'_training_data.pickle' \n",
    "use_model    = 'CANNON/'+DR+'/'+output+'/'+output+mode_in+subset+'_model.pickle'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cannon definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nonlinear_invert(f, sigmas, coeffs, scatters,labels): \n",
    "    xdata = np.vstack([coeffs])\n",
    "    sigmavals = np.sqrt(sigmas ** 2 + scatters ** 2) \n",
    "    guessit = [0]*len(labels)\n",
    "    try: \n",
    "        model, cov = opt.curve_fit(_func, xdata, f, sigma = sigmavals, maxfev=18000, p0 = guessit)\n",
    "    except RuntimeError:\n",
    "        model = [999]*len(labels)\n",
    "        cov = np.ones((len(labels),len(labels) ))\n",
    "    return model, cov\n",
    "\n",
    "def _func(coeffs, *labels):\n",
    "    \"\"\" Takes the dot product of coefficients vec & labels vector \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coeffs: numpy ndarray\n",
    "        the coefficients on each element of the label vector\n",
    "    *labels: numpy ndarray\n",
    "        label vector\n",
    "    Returns\n",
    "    -------\n",
    "    dot product of coeffs vec and labels vec\n",
    "    \"\"\"\n",
    "    nlabels = len(labels)\n",
    "    linear_terms = labels\n",
    "    quadratic_terms = np.outer(linear_terms, linear_terms)[np.triu_indices(nlabels)]\n",
    "    lvec = np.hstack((linear_terms, quadratic_terms))\n",
    "    return np.dot(coeffs[:,1:], lvec)\n",
    "\n",
    "\n",
    "def infer_labels_nonlinear(fn_pickle,testdata, ids, fout_pickle):\n",
    "    file_in = open(fn_pickle, 'r') \n",
    "    dataall, metaall, labels, offsets, coeffs, covs, scatters,chis,chisq = pickle.load(file_in)\n",
    "    file_in.close()\n",
    "    nstars = (testdata.shape)[1]\n",
    "    nlabels = len(labels)\n",
    "    print '......Infering labels for '+str(np.shape(testdata))+' shaped data of '+str(nstars)+' stars with '+str(nlabels)+' labels'\n",
    "    Params_all = np.zeros((nstars, nlabels))\n",
    "    MCM_rotate_all = np.zeros((nstars, np.shape(coeffs)[1]-1, np.shape(coeffs)[1]-1))\n",
    "    covs_all = np.zeros((nstars,nlabels, nlabels))\n",
    "    errs_all = np.zeros((nstars,nlabels))\n",
    "    for jj in range(0,nstars):\n",
    "        if np.any(abs(testdata[:,jj,0] - dataall[:, 0, 0]) > 0.0001): \n",
    "            print testdata[range(5),jj,0], dataall[range(5),0,0]\n",
    "            assert False\n",
    "        xdata = testdata[:,jj,0]\n",
    "        ydata = testdata[:,jj,1]\n",
    "        ysigma = testdata[:,jj,2]\n",
    "        ydata_norm = ydata  - coeffs[:,0] # subtract the mean \n",
    "        f = ydata_norm \n",
    "        Cinv = 1. / (ysigma ** 2 + scatters ** 2)\n",
    "        Params,covs = nonlinear_invert(f, 1/Cinv**0.5 ,coeffs, scatters,labels) \n",
    "        Params = Params+offsets\n",
    "        num_cut = -1*(np.shape(coeffs)[-1] -1) \n",
    "        coeffs_slice = coeffs[:,num_cut:]\n",
    "        MCM_rotate = np.dot(coeffs_slice.T, Cinv[:,None] * coeffs_slice)\n",
    "        Params_all[jj,:] = Params \n",
    "        MCM_rotate_all[jj,:,:] = MCM_rotate \n",
    "        covs_all[jj,:,:] = covs\n",
    "        errs_all[jj,:] = covs.diagonal()\n",
    "    filein = fout_pickle.split('_tags') [0] \n",
    "    if np.logical_or(filein == 'CANNON/'+DR+'/'+output+'/'+output+mode_in+subset+'_selftest', filein[0:8] == 'selftest'):\n",
    "        print '......Confirmed: Selftest'\n",
    "        file_in = open(fout_pickle, 'w')  \n",
    "        file_normed = trainingset.split('.pickle')[0]\n",
    "        chi2,chi2_good,chi2_each = get_goodness_fit(fn_pickle, file_normed, Params_all )\n",
    "        chi2_def = chi2 # this will be the not reduced chi2 #  /(len(xdata)*1. - 3.) #7209. #8575-363-3 # len(xdata)*1.\n",
    "        #good_pixels = map( lambda goodi: len(np.where(testdata[:,goodi,2]<=1.)[0]),range(0,len(testdata[0,:,2])))      \n",
    "        #chi2_def2 = chi2_def/good_pixels\n",
    "        pickle.dump((Params_all, errs_all, covs_all,chi2_def,ids,chi2_good,chi2_each),  file_in)\n",
    "        file_in.close()\n",
    "    else:\n",
    "        file_in = open(fout_pickle, 'w')\n",
    "        file_normed = testdataname.split('.pickle')[0]\n",
    "        chi2,chi2_good,chi2_each = get_goodness_fit(fn_pickle, file_normed, Params_all )\n",
    "        chi2_def = chi2 # this will be the not reduced chi2 #  /(len(xdata)*1. - 3.) #7209. #8575-363-3 # len(xdata)*1.\n",
    "        #good_pixels = map( lambda goodi: len(np.where(testdata[:,goodi,2]<=1.)[0]),range(0,len(testdata[0,:,2])))\n",
    "        #print good_pixels\n",
    "        #print chi2_def\n",
    "        #chi2_def2 = chi2_def/good_pixels\n",
    "        #print chi2_def2\n",
    "        pickle.dump((Params_all, errs_all, covs_all,chi2_def,ids,chi2_good,chi2_each),  file_in)\n",
    "        file_in.close()\n",
    "    return Params_all , MCM_rotate_all\n",
    "\n",
    "def get_goodness_fit(fn_pickle, filein, Params_all ):\n",
    "    print '......Get goodness of git for '+str(fn_pickle)+' fielin: '+str(filein)+' with parameters '+str(np.shape(Params_all))\n",
    "    fd = open(fn_pickle,'r')\n",
    "    dataall, metaall, labels, offsets, coeffs, covs, scatters, chis, chisq = pickle.load(fd) \n",
    "    fd.close() \n",
    "    file_with_star_data = str(filein)+\".pickle\"\n",
    "    #file_with_star_data = \"testdata.pickle\"\n",
    "    file_normed = trainingset.split('.pickle')[0]\n",
    "    if filein != file_normed: \n",
    "        #print 'filein != file_normed'\n",
    "        f_flux= open(file_with_star_data, 'r') \n",
    "        flux,num,num2 = pickle.load(f_flux) \n",
    "    if filein == file_normed: \n",
    "        #print 'filein == file_normed'\n",
    "        f_flux = open(trainingset, 'r') \n",
    "        flux, filterall, metaall, labelthings,  cluster_name, ids = pickle.load(f_flux)\n",
    "    f_flux.close() \n",
    "    labels = Params_all \n",
    "    nlabels = np.shape(labels)[1]\n",
    "    nstars = np.shape(labels)[0]\n",
    "    features_data = np.ones((nstars, 1))\n",
    "    offsets = np.mean(labels, axis = 0) \n",
    "    features_data = np.hstack((features_data, labels - offsets)) \n",
    "    newfeatures_data = np.array([np.outer(m, m)[np.triu_indices(nlabels)] for m in (labels - offsets)])\n",
    "    features_data = np.hstack((features_data, newfeatures_data)) \n",
    "    chi2_all = np.zeros(nstars) \n",
    "    chi2_all_good = np.zeros(nstars)\n",
    "    chi2_all_each = np.zeros((nstars,len(flux[:,0,2])))\n",
    "    for jj in range(nstars):\n",
    "        model_gen = np.dot(coeffs,features_data.T[:,jj]) \n",
    "        data_star = flux[:,jj,1] \n",
    "        Cinv = 1. / (flux[:,jj, 2] ** 2 + scatters ** 2)  # invvar slice of data\n",
    "        chi2_each = (Cinv) * (data_star - np.dot(coeffs, features_data.T[:,jj]))**2\n",
    "        chi2 = sum(chi2_each)\n",
    "        chi2_good = chi2/len(np.where(np.array(flux[:,jj,2])<=1.)[0])\n",
    "        chi2_all[jj] = chi2\n",
    "        chi2_all_good[jj] = chi2_good\n",
    "        chi2_all_each[jj,:] = chi2_each\n",
    "    return(chi2_all,chi2_all_good,chi2_all_each)\n",
    "\n",
    "def do_regressions(dataall, filterall, features):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    nlam, nobj, ndata = dataall.shape\n",
    "    nobj, npred = features.shape\n",
    "    featuresall = np.zeros((nlam,nobj,npred))\n",
    "    featuresall[:, :, :] = features[None, :, :]\n",
    "    return map(do_one_regression, dataall, filterall, featuresall)\n",
    "\n",
    "def do_one_regression_at_fixed_scatter(data, filter1, features, scatter):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: ndarray, [nobjs, 2]\n",
    "        fluxes, invvars\n",
    "    meta: ndarray, [nobjs, nmeta]\n",
    "        Teff, Feh, etc, etc\n",
    "    scatter:\n",
    "    Returns\n",
    "    -------\n",
    "    coeff: ndarray\n",
    "        coefficients of the fit\n",
    "    MTCinvM: ndarray\n",
    "        inverse covariance matrix for fit coefficients\n",
    "    chi: float\n",
    "        chi-squared at best fit\n",
    "    logdet_Cinv: float\n",
    "        inverse of the log determinant of the cov matrice\n",
    "        :math:`\\sum(\\log(Cinv))`\n",
    "    use the same terminology as in the paper \n",
    "    \"\"\"\n",
    "    #scatter = kwargs.get('scatter', 0)\n",
    "    nmeta = filter1.shape[0]\n",
    "    nobjs, npars = features.shape\n",
    "    assert npars == nmeta * (nmeta + 3) / 2 + 1\n",
    "    #print filter1.shape,nmeta, features.shape\n",
    "    filter_features = [np.hstack((1, filter1)) ]\n",
    "    filter1 = np.array([filter1])\n",
    "    # the way the filters is done is repeating code that exists in train()\n",
    "    filter_newfeatures = np.array([np.outer(m, m)[np.triu_indices(nmeta)] for m in filter1])\n",
    "    filter_features = np.hstack((filter_features, filter_newfeatures))[0]\n",
    "    filter_features = np.array(filter_features)\n",
    "    filter_features_bool = filter_features.astype(bool)\n",
    "    assert np.shape(filter_features_bool)[0] == npars\n",
    "    D = np.sum(filter1)\n",
    "    assert np.sum(filter_features_bool) == D * (D + 3) / 2 + 1\n",
    "    ### make the filter above\n",
    "    # least square fit\n",
    "    Cinv = 1. / (data[:,1] ** 2 + scatter ** 2)  # invvar slice of data\n",
    "    M = features[:,filter_features_bool]\n",
    "    MTCinvM = np.dot(M.T, Cinv[:, None] * M) # craziness b/c Cinv isnt a matrix\n",
    "    x = data[:,0] # intensity slice of data\n",
    "    MTCinvx = np.dot(M.T, Cinv * x)\n",
    "    coeff_full = np.zeros(len(filter_features))\n",
    "    coeff_ind = np.arange(len(filter_features))[filter_features_bool]\n",
    "    try:\n",
    "        coeff = np.linalg.solve(MTCinvM, MTCinvx)\n",
    "    except np.linalg.linalg.LinAlgError:\n",
    "        print MTCinvM, MTCinvx, data[:,0], data[:,1]\n",
    "        print features\n",
    "    if not np.all(np.isfinite(coeff)):\n",
    "        print \"coefficients not finite\"\n",
    "        print coeff, median(data[:,1]), data.shape , scatter\n",
    "        assert False\n",
    "    for a,b in zip(coeff_ind ,coeff):\n",
    "        coeff_full[a] = b\n",
    "    chi = np.sqrt(Cinv) * (x - np.dot(M, coeff))\n",
    "    logdet_Cinv = np.sum(np.log(Cinv))\n",
    "    return (coeff_full, MTCinvM, chi, logdet_Cinv )\n",
    "\n",
    "def do_one_regression(data, filter1, metadata):\n",
    "    \"\"\"\n",
    "    does a regression at a single wavelength to fit calling the fixed scatter routine\n",
    "    # inputs:\n",
    "    \"\"\"\n",
    "    ln_s_values = np.arange(np.log(0.01), 0., 10.4)\n",
    "    chis_eval = np.zeros_like(ln_s_values)\n",
    "    for ii, ln_s in enumerate(ln_s_values):\n",
    "        foo, bar, chi, logdet_Cinv = do_one_regression_at_fixed_scatter(data, filter1, metadata, scatter = np.exp(ln_s))\n",
    "        chis_eval[ii] = np.sum(chi * chi) - logdet_Cinv\n",
    "    if np.any(np.isnan(chis_eval)):\n",
    "        s_best = np.exp(ln_s_values[-1])\n",
    "        return do_one_regression_at_fixed_scatter(data, filter1, metadata, scatter = s_best) + (s_best, )\n",
    "    lowest = np.argmin(chis_eval)\n",
    "    if lowest == 0 or lowest == len(ln_s_values)-1:\n",
    "        s_best = np.exp(ln_s_values[lowest])\n",
    "        return do_one_regression_at_fixed_scatter(data, filter1, metadata, scatter = s_best) + (s_best, )\n",
    "\n",
    "def get_normalized_training_data_firstcall(testfile_in):\n",
    "    if glob.glob(trainingset):\n",
    "        file_in2 = open(trainingset, 'r')\n",
    "        dataall, filterall, metaall, labels, name_take, name_galah_id= pickle.load(file_in2)\n",
    "        file_in2.close()\n",
    "        return dataall, filterall, metaall, labels, name_take, name_galah_id\n",
    "\n",
    "def train(dataall, filterall, metaall, order, fn, cluster_name, leave_out=None):\n",
    "    \"\"\"\n",
    "    - `leave out` must be in the correct form to be an input to `np.delete`\n",
    "    \"\"\"\n",
    "    if leave_out is not None: #\n",
    "        dataall = np.delete(dataall, [leave_out], axis = 1)\n",
    "        metaall = np.delete(metaall, [leave_out], axis = 0)\n",
    "    nstars, nmeta = metaall.shape\n",
    "    offsets = np.mean(metaall, axis=0)\n",
    "    features = np.ones((nstars, 1))\n",
    "    if order >= 1:\n",
    "        features = np.hstack((features, metaall - offsets))\n",
    "    if order >= 2:\n",
    "        newfeatures = np.array([np.outer(m, m)[np.triu_indices(nmeta)] for m in (metaall - offsets)])\n",
    "        features = np.hstack((features, newfeatures))\n",
    "    blob = do_regressions(dataall, filterall, features)\n",
    "    coeffs = np.array([b[0] for b in blob])\n",
    "    covs = np.array([np.linalg.inv(b[1]) for b in blob])\n",
    "    chis = np.array([b[2] for b in blob])\n",
    "    chisqs = np.array([np.dot(b[2],b[2]) - b[3] for b in blob]) # holy crap be careful\n",
    "    scatters = np.array([b[4] for b in blob])\n",
    "    fd = open(fn, \"w\")\n",
    "    pickle.dump((dataall, metaall, labels, offsets, coeffs, covs, scatters,chis,chisqs), fd)\n",
    "    fd.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform TRAIN if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if obs_date == '':\n",
    "    if not glob.glob(use_model):\n",
    "        print('WILL PERFORM TRAINING')\n",
    "\n",
    "        print('IMPORT TRAININGSET')\n",
    "        dataall, filterall, metaall, labels, galah_name, galah_id = get_normalized_training_data_firstcall(output)\n",
    "\n",
    "        print('START TRAINING')\n",
    "        train(dataall, filterall, metaall, 2, use_model,  galah_name)\n",
    "\n",
    "        print('START SELFVALIDATION')\n",
    "        a = open(trainingset, 'r') \n",
    "        testdataall, filterall, metadata, labels, ids, galah_name = pickle.load(a) \n",
    "        filterall[:] = 1.\n",
    "        a.close()\n",
    "        fieldself = 'CANNON/'+DR+'/'+output+'/'+output+mode_in+subset+'_selftest'\n",
    "        testmetaall, inv_covars = infer_labels_nonlinear(use_model, testdataall, ids, fieldself+\"_tags.pickle\") \n",
    "        dataall, filterall, metaall, labels, galah_name, galah_id = get_normalized_training_data_firstcall(output)\n",
    "\n",
    "        print('START LEAVE 20% OUT')\n",
    "        take_out=np.arange(len(galah_name))\n",
    "        np.random.shuffle(take_out)\n",
    "        steps=5\n",
    "        sz=len(galah_name)/steps # for 20% take out\n",
    "        parts=[]\n",
    "        for steping in range(0,steps):\n",
    "            if steping==0:\n",
    "                parts.append(take_out[0:sz])\n",
    "            else:\n",
    "                if steping==steps-1:\n",
    "                    parts.append(take_out[(steps-1)*sz:len(galah_name)])\n",
    "                else:\n",
    "                    parts.append(take_out[steping*sz:(steping+1)*sz])\n",
    "\n",
    "        for each in range(0,steps):\n",
    "            print('STARTING LEAVE_OUT TEST '+str(each+1)+'/'+str(steps))\n",
    "            print('START TRAINING PART '+str(each+1)+'/'+str(steps))\n",
    "            np.savetxt('CANNON/'+DR+'/'+output+'/leaveouttest/'+output+mode_in+subset+'_shuffled_order_'+str(each+1)+'.txt',parts[each],fmt='%s')\n",
    "            train(dataall,filterall,metaall,2,'CANNON/'+DR+'/'+output+'/leaveouttest/'+output+mode_in+subset+'_out'+str(each+1)+'_model.pickle',galah_name[parts[each]],leave_out=parts[each])\n",
    "            print('START TEST '+str(each+1)+'/'+str(steps))\n",
    "            fieldself = 'CANNON/'+DR+'/'+output+'/leaveouttest/'+output+mode_in+subset+'_out'\n",
    "            #dataall_part= np.delete(dataall, parts[each], axis = 1)\n",
    "            testdataname=trainingset\n",
    "            testmetaall, inv_covars = infer_labels_nonlinear('CANNON/'+DR+'/'+output+'/leaveouttest/'+output+mode_in+subset+'_out'+str(each+1)+'_model.pickle', dataall, galah_name, fieldself+str(each+1)+'.pickle',0.00,1.40)\n",
    "\n",
    "        import email\n",
    "        import email.mime.application\n",
    "        from email.MIMEMultipart import MIMEMultipart\n",
    "        from email.MIMEText import MIMEText\n",
    "        from email.MIMEImage import MIMEImage\n",
    "        msg = MIMEMultipart()\n",
    "\n",
    "        msg['From'] = 'gemini2'\n",
    "        msg['To'] = 'buder@mpia.de'\n",
    "        msg['Subject'] = 'TRAIN finished for '+output+mode_in+' '+DR\n",
    "\n",
    "    #     filename='Cannon1.3_SMEmasks_selftest_HRDs.pdf'\n",
    "    #     fp=open('CANNON/'+DR+'/'+output+'/diagnostic_plots/'+filename,'rb')\n",
    "    #     att = email.mime.application.MIMEApplication(fp.read(),_subtype=\"pdf\")\n",
    "    #     fp.close()\n",
    "    #     att.add_header('Content-Disposition','attachment',filename=filename)\n",
    "    #     msg.attach(att)\n",
    "\n",
    "        import smtplib\n",
    "        mailer = smtplib.SMTP('localhost')\n",
    "        mailer.sendmail('gemini2', 'buder@mpia.de', msg.as_string())\n",
    "        mailer.close()\n",
    "        print('Email sent')\n",
    "    else:\n",
    "        print('TRAINING SET EXISTS')\n",
    "else:\n",
    "    print('USE EXISTING TRAINING SET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do some quick diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# door = open(use_model, 'r')\n",
    "# used_model = pickle.load(door)\n",
    "# door.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform TEST step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if obs_date != '':\n",
    "\n",
    "    print('WORKING ON TESTSET PART '+str(obs_date))\n",
    "    testdataname='CANNON/'+DR+'/pickle_'+DR+'_'+ccds+'ccds/'+obs_date\n",
    "    saveas = 'CANNON/'+DR+'/'+output+'/pickle_test/'+output+mode_in+subset+'_'+obs_date[:-7]+'_tags.pickle'\n",
    "\n",
    "    if not glob.glob(saveas):\n",
    "\n",
    "        a=open(testdataname,'r')\n",
    "        testdataall, ids, galah_name = pickle.load(a)\n",
    "        a.close()\n",
    "        print('START TEST PART '+output+' '+subset+' '+DR+' '+str(obs_date))\n",
    "        testmetaall, inv_covars = infer_labels_nonlinear(use_model, testdataall, ids, saveas)\n",
    "    else:\n",
    "        print('TESTSET ALREADY DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert IPYNB to PY\n",
    "\n",
    "os.chdir('/shared-storage/buder/svn-repos/trunk/GALAH/TheGALAHCannon/')\n",
    "\n",
    "convert_command = 'ipython nbconvert --to script Cannon_train.ipynb'\n",
    "os.system(convert_command)\n",
    "\n",
    "os.chdir('/shared-storage/buder/svn-repos/trunk/GALAH/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
