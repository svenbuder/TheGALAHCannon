{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cannon_collect_test\n",
    "\n",
    "This routine is collecting the data from the training set run\n",
    "\n",
    "Author(s): Sven Buder\n",
    "\n",
    "History:\n",
    "171020: SB Create file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compatibility with Python 3\n",
    "from __future__ import (absolute_import, division, print_function)\n",
    "\n",
    "try:\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format='retina'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Basic packages\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "# Packages to work with FITS and (IDL) SME.out files\n",
    "import astropy.io.fits as pyfits\n",
    "import astropy.table as table\n",
    "from scipy.io.idl import readsav\n",
    "\n",
    "# Matplotlib and associated packages for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "params = {\n",
    "    'font.family'        : 'lmodern',\n",
    "    'font.size'          : 17,\n",
    "    'axes.labelsize'     : 20,\n",
    "    'ytick.labelsize'    : 16,\n",
    "    'xtick.labelsize'    : 16,\n",
    "    'legend.fontsize'    : 20,\n",
    "    'text.usetex'        : True, \n",
    "    'text.latex.preamble': [r'\\usepackage{upgreek}', r'\\usepackage{amsmath}'],\n",
    "    }   \n",
    "plt.rcParams.update(params)\n",
    "\n",
    "_parula_data = [[0.2081, 0.1663, 0.5292], \n",
    "                [0.2116238095, 0.1897809524, 0.5776761905], \n",
    "                [0.212252381, 0.2137714286, 0.6269714286], \n",
    "                [0.2081, 0.2386, 0.6770857143], \n",
    "                [0.1959047619, 0.2644571429, 0.7279], \n",
    "                [0.1707285714, 0.2919380952, 0.779247619], \n",
    "                [0.1252714286, 0.3242428571, 0.8302714286], \n",
    "                [0.0591333333, 0.3598333333, 0.8683333333], \n",
    "                [0.0116952381, 0.3875095238, 0.8819571429], \n",
    "                [0.0059571429, 0.4086142857, 0.8828428571], \n",
    "                [0.0165142857, 0.4266, 0.8786333333], \n",
    "                [0.032852381, 0.4430428571, 0.8719571429], \n",
    "                [0.0498142857, 0.4585714286, 0.8640571429], \n",
    "                [0.0629333333, 0.4736904762, 0.8554380952], \n",
    "                [0.0722666667, 0.4886666667, 0.8467], \n",
    "                [0.0779428571, 0.5039857143, 0.8383714286], \n",
    "                [0.079347619, 0.5200238095, 0.8311809524], \n",
    "                [0.0749428571, 0.5375428571, 0.8262714286], \n",
    "                [0.0640571429, 0.5569857143, 0.8239571429], \n",
    "                [0.0487714286, 0.5772238095, 0.8228285714], \n",
    "                [0.0343428571, 0.5965809524, 0.819852381], \n",
    "                [0.0265, 0.6137, 0.8135], \n",
    "                [0.0238904762, 0.6286619048, 0.8037619048], \n",
    "                [0.0230904762, 0.6417857143, 0.7912666667], \n",
    "                [0.0227714286, 0.6534857143, 0.7767571429], \n",
    "                [0.0266619048, 0.6641952381, 0.7607190476], \n",
    "                [0.0383714286, 0.6742714286, 0.743552381], \n",
    "                [0.0589714286, 0.6837571429, 0.7253857143], \n",
    "                [0.0843, 0.6928333333, 0.7061666667], \n",
    "                [0.1132952381, 0.7015, 0.6858571429], \n",
    "                [0.1452714286, 0.7097571429, 0.6646285714], \n",
    "                [0.1801333333, 0.7176571429, 0.6424333333], \n",
    "                [0.2178285714, 0.7250428571, 0.6192619048], \n",
    "                [0.2586428571, 0.7317142857, 0.5954285714], \n",
    "                [0.3021714286, 0.7376047619, 0.5711857143], \n",
    "                [0.3481666667, 0.7424333333, 0.5472666667], \n",
    "                [0.3952571429, 0.7459, 0.5244428571], \n",
    "                [0.4420095238, 0.7480809524, 0.5033142857], \n",
    "                [0.4871238095, 0.7490619048, 0.4839761905], \n",
    "                [0.5300285714, 0.7491142857, 0.4661142857], \n",
    "                [0.5708571429, 0.7485190476, 0.4493904762],\n",
    "                [0.609852381, 0.7473142857, 0.4336857143], \n",
    "                [0.6473, 0.7456, 0.4188], \n",
    "                [0.6834190476, 0.7434761905, 0.4044333333], \n",
    "                [0.7184095238, 0.7411333333, 0.3904761905], \n",
    "                [0.7524857143, 0.7384, 0.3768142857], \n",
    "                [0.7858428571, 0.7355666667, 0.3632714286], \n",
    "                [0.8185047619, 0.7327333333, 0.3497904762], \n",
    "                [0.8506571429, 0.7299, 0.3360285714], \n",
    "                [0.8824333333, 0.7274333333, 0.3217], \n",
    "                [0.9139333333, 0.7257857143, 0.3062761905], \n",
    "                [0.9449571429, 0.7261142857, 0.2886428571], \n",
    "                [0.9738952381, 0.7313952381, 0.266647619], \n",
    "                [0.9937714286, 0.7454571429, 0.240347619], \n",
    "                [0.9990428571, 0.7653142857, 0.2164142857], \n",
    "                [0.9955333333, 0.7860571429, 0.196652381], \n",
    "                [0.988, 0.8066, 0.1793666667], \n",
    "                [0.9788571429, 0.8271428571, 0.1633142857], \n",
    "                [0.9697, 0.8481380952, 0.147452381], \n",
    "                [0.9625857143, 0.8705142857, 0.1309], \n",
    "                [0.9588714286, 0.8949, 0.1132428571], \n",
    "                [0.9598238095, 0.9218333333, 0.0948380952], \n",
    "                [0.9661, 0.9514428571, 0.0755333333], \n",
    "                [0.9763, 0.9831, 0.0538]]\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "parula = ListedColormap(_parula_data, name='parula')\n",
    "parula_zero = _parula_data[0]\n",
    "\n",
    "# Package to save multiple PDF pages in one PDF\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /shared-storage/buder/svn-repos/trunk/GALAH\n"
     ]
    }
   ],
   "source": [
    "# Change Work directory (if Sven's computer)\n",
    "try:\n",
    "    localFilePath = '/shared-storage/buder/svn-repos/trunk/GALAH/'\n",
    "    os.chdir(localFilePath)\n",
    "    print('Current working directory: '+os.getcwd())\n",
    "except:\n",
    "    print('Could not change Path to '+localFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tex_dict = dict(\n",
    "    Teff     = r'$T_\\mathrm{eff}~\\mathrm{[K]}$',\n",
    "    Logg     = r'$\\log g~\\mathrm{[dex]}$',\n",
    "    Feh      = r'$\\mathrm{[Fe/H]~[dex]}$',\n",
    "    Vmic     = r'$v_\\mathrm{mic}~\\mathrm{[km/s]}$',\n",
    "    Vsini    = r'$v_\\mathrm{broad}~\\mathrm{[km/s]}$',\n",
    "    Alpha_fe = r'$\\mathrm{[\\alpha/Fe]~[dex]}$',\n",
    "    XFe      = r'$\\mathrm{[X/Fe]~[dex]}$',\n",
    "    SNR      = r'$\\mathrm{Green~Channel~S/N}$',\n",
    "    Li        = r'$\\mathrm{[Li/Fe]~[dex]}$',\n",
    "    C        = r'$\\mathrm{[C/Fe]~[dex]}$',\n",
    "    O        = r'$\\mathrm{[O/Fe]~[dex]}$',\n",
    "    Na       = r'$\\mathrm{[Na/Fe]~[dex]}$',\n",
    "    Mg       = r'$\\mathrm{[Mg/Fe]~[dex]}$',\n",
    "    Al       = r'$\\mathrm{[Al/Fe]~[dex]}$',\n",
    "    Si       = r'$\\mathrm{[Si/Fe]~[dex]}$',\n",
    "    K        = r'$\\mathrm{[K/Fe]~[dex]}$',\n",
    "    Ca       = r'$\\mathrm{[Ca/Fe]~[dex]}$',\n",
    "    Sc       = r'$\\mathrm{[Sc/Fe]~[dex]}$',\n",
    "    Ti       = r'$\\mathrm{[Ti/Fe]~[dex]}$',\n",
    "    V        = r'$\\mathrm{[V/Fe]~[dex]}$',\n",
    "    Cr       = r'$\\mathrm{[Cr/Fe]~[dex]}$',\n",
    "    Mn       = r'$\\mathrm{[Mn/Fe]~[dex]}$',\n",
    "    Co       = r'$\\mathrm{[Co/Fe]~[dex]}$',\n",
    "    Ni       = r'$\\mathrm{[Ni/Fe]~[dex]}$',\n",
    "    Zn       = r'$\\mathrm{[Zn/Fe]~[dex]}$',\n",
    "    Rb       = r'$\\mathrm{[Rb/Fe]~[dex]}$',\n",
    "    Sr       = r'$\\mathrm{[Sr/Fe]~[dex]}$',\n",
    "    Y        = r'$\\mathrm{[Y/Fe]~[dex]}$',\n",
    "    Zr       = r'$\\mathrm{[Zr/Fe]~[dex]}$',\n",
    "    Mo       = r'$\\mathrm{[Mo/Fe]~[dex]}$',\n",
    "    Ru       = r'$\\mathrm{[Ru/Fe]~[dex]}$',\n",
    "    Ba       = r'$\\mathrm{[Ba/Fe]~[dex]}$',\n",
    "    La       = r'$\\mathrm{[La/Fe]~[dex]}$',\n",
    "    Ce       = r'$\\mathrm{[Ce/Fe]~[dex]}$',\n",
    "    Nd       = r'$\\mathrm{[Nd/Fe]~[dex]}$',\n",
    "    Sm       = r'$\\mathrm{[Sm/Fe]~[dex]}$',\n",
    "    Eu       = r'$\\mathrm{[Eu/Fe]~[dex]}$'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def export_fits(dictionary, filename):\n",
    "    \"\"\"\n",
    "    This is a function to export a datastructure, nomatter where you currently are\n",
    "    \n",
    "    INPUT:\n",
    "    dictionary : dictionary data_structure\n",
    "    filename   : how should the file be called? Best: include the full path\n",
    "    \n",
    "    OUTPUT:\n",
    "    FITS file saved as filename\n",
    "    \"\"\"\n",
    "    \n",
    "    t = table.Table()\n",
    "    for each_key in dictionary.keys():\n",
    "        t.add_column(table.Column(name=each_key, data=dictionary[each_key]))\n",
    "    t.write(filename+'.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_galah_dict(wg3_wg4_setup):\n",
    "    galah_dict = dict(\n",
    "\n",
    "        # General\n",
    "        sobject_id     = 112233000001234,\n",
    "        galah_id       = 1234567,\n",
    "        field_id       = 0,\n",
    "        ra             = 0.0,\n",
    "        dec            = 0.0,\n",
    "        ebv            = 0.0,\n",
    "        snr_c1_iraf    = 0.0,\n",
    "        snr_c2_iraf    = 0.0,\n",
    "        snr_c3_iraf    = 0.0,\n",
    "        snr_c4_iraf    = 0.0,\n",
    "        red_flag       = 0,\n",
    "        \n",
    "        # GUESS\n",
    "        teff_guess     = 0.0,\n",
    "        logg_guess     = 0.0,\n",
    "        feh_guess      = 0.0,\n",
    "        rv_guess       = 0.0,\n",
    "        e_rv_guess     = 0.0,\n",
    "        rv_guess_shift = 0.0,\n",
    "        flag_guess     = 0,\n",
    "\n",
    "    #     # General SME\n",
    "    #     trained_on     = 0,\n",
    "    #     rv_sme         = 0.0,\n",
    "    #     e_rv_sme       = 0.0,\n",
    "    #     Alpha_fe_sme   = 0.0,\n",
    "    #     e_Alpha_fe_sme = 0.0,\n",
    "\n",
    "        # General Cannon\n",
    "        chi2_cannon       = 0.0,\n",
    "        sp_label_distance = 0.0,\n",
    "        flag_cannon       = 0,\n",
    "        Alpha_fe_cannon   = 0.0,\n",
    "        e_Alpha_fe_cannon = 0.0\n",
    "        )\n",
    "\n",
    "    #for each_method in ['sme','cannon']:\n",
    "    for each_method in ['cannon']:\n",
    "        for each_sp in wg3_wg4_setup['stellar_parameters']:\n",
    "            galah_dict[each_sp+'_'+each_method] = 0.0\n",
    "            galah_dict['e_'+each_sp+'_'+each_method] = 0.0\n",
    "\n",
    "        for each_elem in wg3_wg4_setup['elements']:\n",
    "            galah_dict[each_elem+'_abund_'+each_method]          = 0.0\n",
    "            galah_dict['e_'+each_elem+'_abund_'+each_method]     = 0.0\n",
    "            galah_dict['ld_'+each_elem+'_abund_'+each_method]    = 0.0\n",
    "            galah_dict['flag_'+each_elem+'_abund_'+each_method]  = 0\n",
    "            galah_dict['depth_'+each_elem+'_abund_'+each_method] = 0.0\n",
    "            galah_dict['sn_'+each_elem+'_abund_'+each_method]    = 0.0\n",
    "            galah_dict['chi2_'+each_elem+'_abund_'+each_method]  = 0.0\n",
    "            \n",
    "    return galah_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_model_pickle(filename):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    \n",
    "    filename : pickle filename path\n",
    "    \n",
    "    OUTPUT:\n",
    "    pickle file structure\n",
    "    \n",
    "    \"\"\"\n",
    "    door = open(filename,'r')\n",
    "    print('Will read pickle file: '+filename)\n",
    "    model_data = pickle.load(door)\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_HRD(dictionary, sme_cannon='cannon', savefig=False, xlim=(7900,3600), ylim=(5.,0.)):\n",
    "    HRD_kwargs = dict(cmap=parula, s=20, lw=0.1, rasterized=True)\n",
    "    f, (ax1,ax2) = plt.subplots(1,2,figsize=(2*6.4, 4.8))\n",
    "\n",
    "    s1 = ax1.scatter(dictionary['Teff_'+sme_cannon],dictionary['Logg_'+sme_cannon],c=dictionary['Feh_'+sme_cannon], vmin=-2.5, vmax=0.5,**HRD_kwargs)\n",
    "    ax1.set_xlabel(tex_dict['Teff'])\n",
    "    ax1.set_ylabel(tex_dict['Logg'])\n",
    "    c1 = plt.colorbar(s1, ax=ax1)\n",
    "    c1.set_label(tex_dict['Feh'])\n",
    "    ax1.set_xlim(xlim)\n",
    "    ax1.set_ylim(ylim)\n",
    "    \n",
    "    s2 = ax2.scatter(dictionary['Teff_'+sme_cannon],dictionary['Logg_'+sme_cannon],c=dictionary['snr_c2_iraf'], vmin=25, vmax=160,**HRD_kwargs)\n",
    "    ax2.set_xlabel(tex_dict['Teff'])\n",
    "    ax2.set_ylabel(tex_dict['Logg'])\n",
    "    c2 = plt.colorbar(s2, ax=ax2)\n",
    "    c2.set_label(tex_dict['SNR'])\n",
    "    ax2.set_xlim(xlim)\n",
    "    ax2.set_ylim(ylim)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if savefig!=False:\n",
    "        plt.savefig(savefig+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ab_scatter(X, Y, ax=plt.gca, **kwargs):\n",
    "    \"\"\"\n",
    "    This function gives back a scatter plot\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    c = kwargs.get('c',parula_zero)\n",
    "    s = kwargs.get('s',2)\n",
    "    s1 = ax.scatter(X,Y,c=c,s=s,alpha=0.5,rasterized=True)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "def ab_dens2d(X, Y, ax=plt.gca, min_per_bin=5, zeroslines=True, interimlines=True, colorbar=True, **kwargs):\n",
    "    \"\"\"\n",
    "    This function gives back a 2D density plot \n",
    "    of the data put in as X and Y with \n",
    "    all points as scatter below certain density\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #first make sure to only use finite X and Y values\n",
    "    XY_finite = (np.isfinite(X) & np.isfinite(Y))\n",
    "    X = X[XY_finite]\n",
    "    Y = Y[XY_finite]\n",
    "    \n",
    "    # General kwargs:\n",
    "    xlabel = kwargs.get('xlabel','')\n",
    "    ylabel = kwargs.get('ylabel', r'$\\mathrm{[X/Fes]~[dex]}$')\n",
    "    xlim   = kwargs.get('xlim', (-3.0,0.65))\n",
    "    ylim   = kwargs.get('ylim', (-0.5,1.00))\n",
    "    cmap = kwargs.get('cmap', parula)\n",
    "    bins = kwargs.get('bins', (0.05,0.025))\n",
    "    if np.shape(bins) != ():\n",
    "        # assuming input in dex\n",
    "        bins = [np.arange(xlim[0],xlim[1],bins[0]),np.arange(ylim[0],ylim[1],bins[1])]\n",
    "    \n",
    "    # plot all points as scatter before density structure is overlaid\n",
    "    scatter = ab_scatter(X,Y,ax=ax)\n",
    "\n",
    "    H, xedges, yedges = np.histogram2d(X,Y,bins=bins)\n",
    "    H=np.rot90(H)\n",
    "    H=np.flipud(H)\n",
    "    Hmasked = np.ma.masked_where(H<min_per_bin,H)\n",
    "\n",
    "    dens2d=ax.pcolormesh(xedges,yedges,Hmasked,cmap=cmap)\n",
    "\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    xticks = kwargs.get('xticks',ax.get_xticks())\n",
    "    ax.set_xticks(xticks)\n",
    "    \n",
    "    if zeroslines == True:\n",
    "        ax.axhline(0,c='k',lw=0.5)\n",
    "        ax.axvline(0,c='k',lw=0.5)\n",
    "        \n",
    "    if interimlines == True:\n",
    "        ax.axhline(0.1,c='k',linestyle='--')\n",
    "        ax.axhline(0.2,c='k',linestyle='--')\n",
    "        ax.axhline(0.3,c='k',linestyle='--')\n",
    "        ax.axhline(0.4,c='k',linestyle='--')\n",
    "        ax.axhline(0.5,c='k',linestyle='--')\n",
    "        ax.axhline(-0.1,c='k',linestyle='--')\n",
    "        ax.axhline(-0.2,c='k',linestyle='--')\n",
    "\n",
    "    if colorbar == True:\n",
    "        c = plt.colorbar(dens2d,ax=ax)\n",
    "        c.set_label('Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_alpha(dictionary, sme_cannon='cannon', savefig=False, xlim=(-3.0,0.65), ylim=(-0.5,1.00)):\n",
    "    \n",
    "    interimlines = False#True\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    for each,mode in enumerate(['Alpha_fe','O','Mg','Si','Ca','Ti']):\n",
    "        ax = plt.subplot(2,3,each+1)\n",
    "        if each == 0:\n",
    "            X = dictionary['Feh_'+sme_cannon]\n",
    "            Y = dictionary['Alpha_fe_'+sme_cannon]\n",
    "        else:\n",
    "            good = dictionary['flag_'+mode+'_abund_'+sme_cannon] == 0\n",
    "            X = dictionary['Feh_'+sme_cannon][good]\n",
    "            Y = dictionary[mode+'_abund_'+sme_cannon][good]\n",
    "        ab_dens2d(\n",
    "            ax=ax,\n",
    "            xlim=xlim,\n",
    "            ylim=ylim,\n",
    "            bins=(0.1,0.05),\n",
    "            X = X,\n",
    "            Y = Y,\n",
    "            xlabel='',\n",
    "            ylabel='',\n",
    "            interimlines=interimlines\n",
    "            );\n",
    "        ax.text(0.05,0.1,tex_dict[mode],transform=ax.transAxes)\n",
    "        if each in [0,3]:\n",
    "            ax.set_ylabel(tex_dict['XFe'])\n",
    "        if each < 3:\n",
    "            ax.set_xticks([])\n",
    "        if each > 2:\n",
    "            ax.set_xlabel(tex_dict['Feh'])\n",
    "        ax.set_xticks(ax.get_xticks()[::2])\n",
    "        ax.set_yticks(ax.get_yticks()[::2])\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if savefig!=False:\n",
    "        plt.savefig(savefig+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_detections(self, mode='Li'):\n",
    "    \"\"\"\n",
    "    This algorithm helps us to decide if an element abundance measurement is\n",
    "    \n",
    "    significant  (flag==0)\n",
    "    upper limit  (flag==1)\n",
    "    bad chi2 fit (flag+2)\n",
    "    \n",
    "    INPUT:\n",
    "    mode : element for which the detections and flags have to be estimated\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # First initialise flags as -1\n",
    "    self.cannon['flag_'+mode+'_abund_cannon'][:] = 4\n",
    "    \n",
    "    # Now we start the flagging for detection/upper limit/bad chi2\n",
    "\n",
    "    # first get the flux/sn within line mask (mob==1 analogous to SME)\n",
    "    mob1    = np.array(self.masks[mode],dtype=bool)\n",
    "    sn_mob1 = np.array(map(lambda x: np.mean(self.spectrum_flux[mode][x,mob1]/self.spectrum_error[mode][x,mob1]), range(len(self.cannon['sobject_id']))))\n",
    "    sn_mob1[sn_mob1 < 1.] = 1.\n",
    "    sn_mob1[np.isnan(sn_mob1)] = 1.\n",
    "    chi2_mob1 = np.array(map(lambda x: np.mean(self.model_chi2[mode][x,mob1]), range(len(self.cannon['sobject_id']))))\n",
    "    \n",
    "    # Upper limit criterium is 2-sigma and minimum line depth of 0.05\n",
    "    upper_limit        = 2./sn_mob1; upper_limit[upper_limit < 0.05] = 0.05\n",
    "    # Upper limit criterium is 3-sigma and minimum line depth of 0.05\n",
    "    significance_limit = 3./sn_mob1; significance_limit[significance_limit < 0.05] = 0.05\n",
    "    \n",
    "    # Line depth within segment, assuming continuum==1.\n",
    "    line_depth = 1. - np.array(map(lambda x: self.spectrum_flux[mode][x,mob1], range(len(self.cannon['sobject_id']))))\n",
    "\n",
    "    self.cannon['depth_'+mode+'_abund_cannon'] = np.array([np.max(line_depth[x]) for x in range(len(line_depth))])\n",
    "    \n",
    "    # Now for each pixel within line mask check if line depth larger than upper limit / significance limit\n",
    "    upper_limit_check        = np.array([line_depth[x,:] >= upper_limit[x] for x in range(len(line_depth[:,0]))])\n",
    "    significance_limit_check = np.array([line_depth[x,:] >= significance_limit[x] for x in range(len(line_depth[:,0]))])\n",
    "\n",
    "    # For upper limit, set flag to 1\n",
    "    self.cannon['flag_'+mode+'_abund_cannon'][np.array([np.any(upper_limit_check[x]) for x in range(len(self.cannon['sobject_id']))])] = 1\n",
    "    # For significant detection, set flag to 0\n",
    "    self.cannon['flag_'+mode+'_abund_cannon'][np.array([np.any(significance_limit_check[x]) for x in range(len(self.cannon['sobject_id']))])] = 0\n",
    "\n",
    "    # Set flag, if chi2 within line mask is above typical value for given S/N within line mask\n",
    "    self.cannon['sn_'+mode+'_abund_cannon']   = sn_mob1\n",
    "    self.cannon['chi2_'+mode+'_abund_cannon'] = chi2_mob1\n",
    "    \n",
    "    # Here we should adjust the S/N depending function\n",
    "    def chi2_limit(log_sn = np.log10(sn_mob1)):\n",
    "        return 4. + 2. * log_sn\n",
    "\n",
    "    #plt.plot(np.log10(sn_mob1), np.log10(chi2_mob1), 'ko')\n",
    "    #plt.plot(np.arange(0.0,3.0,0.1), chi2_limit(np.arange(0.0,3.0,0.1)), 'r')\n",
    "        \n",
    "    chi2_high = (chi2_mob1 > chi2_limit(log_sn = np.log10(sn_mob1)))\n",
    "    self.cannon['flag_'+mode+'_abund_cannon'][chi2_high] += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_distance(self, mode='Sp'):\n",
    "    \"\"\"\n",
    "    Here we flag those solutions far away from the Training set\n",
    "    (similar to LAMOST's D from Ho et al. 2016)\n",
    "    \"\"\"\n",
    "\n",
    "    precision = np.load(localFilePath+'CANNON/'+self.reduction_DR+'/'+self.version_cannon+'/fits_files/'+self.version_cannon+'_'+self.setup_cannon+'_'+self.reduction_DR+'_'+str(self.nr_ccds_cannon)+'ccds_precision.npy').item()\n",
    "\n",
    "    t = table.Table()\n",
    "    ts = t.read(localFilePath+'CANNON/'+self.reduction_DR+'/'+self.version_cannon+'/trainingset/'+self.version_cannon+'_'+mode+'_'+self.setup_cannon+'_trainingset.fits')\n",
    "\n",
    "    if mode == 'Sp':\n",
    "        D_part = np.zeros(\n",
    "            (len(ts['sobject_id']), len(self.cannon['sobject_id'])),\n",
    "            dtype = ([('Teff',float),('Logg',float),('Feh',float),('Vsini',float)]))\n",
    "    else:\n",
    "        D_part = np.zeros(\n",
    "            (len(ts['sobject_id']), len(self.cannon['sobject_id'])),\n",
    "            dtype = ([('Teff',float),('Logg',float),('Feh',float),('Vsini',float),(mode,float)]))\n",
    "\n",
    "    D_part['Teff']  = map(lambda x: 1/(precision['cannon_rms']['Teff'] )**2. * (self.cannon['Teff_cannon'] - ts['Teff_sme'][x])**2.,range(len(ts['sobject_id'])))\n",
    "    D_part['Logg']  = map(lambda x: 1/(precision['cannon_rms']['Logg'] )**2. * (self.cannon['Logg_cannon'] - ts['Logg_sme'][x])**2.,range(len(ts['sobject_id'])))\n",
    "    D_part['Feh']   = map(lambda x: 1/(precision['cannon_rms']['Feh']  )**2. * (self.cannon['Feh_cannon'] - ts['Feh_sme'][x])**2.,range(len(ts['sobject_id'])))\n",
    "    D_part['Vsini'] = map(lambda x: 1/(precision['cannon_rms']['Vsini'])**2. * (self.cannon['Vsini_cannon'] - ts['Vsini_sme'][x])**2.,range(len(ts['sobject_id'])))\n",
    "    if mode == 'Sp':\n",
    "        D_sum = D_part['Teff'] + D_part['Logg'] + D_part['Feh'] + D_part['Vsini']\n",
    "    else:\n",
    "        D_part[mode] = map(lambda x: 1/(precision['cannon_rms'][mode] )**2. * (self.cannon[mode+'_abund_cannon'] - ts[mode+'_abund_sme'][x])**2.,range(len(ts['sobject_id'])))\n",
    "        D_sum = D_part['Teff'] + D_part['Logg'] + D_part['Feh'] + D_part['Vsini'] + D_part[mode]\n",
    "\n",
    "    # We follow Ho et al. 2016 by calculating the mean of the 10 smallest label-distances\n",
    "    D = np.array(map(lambda x: np.mean(D_sum[:,x][D_sum[:,x].argsort()[:10]]),range(len(self.cannon['sobject_id']))))\n",
    "    plt.figure()\n",
    "    plt.hist(D[np.isfinite(D)],bins=np.arange(0,21,0.25));\n",
    "    plt.xlabel('D ('+mode+')')\n",
    "    plt.savefig(localFilePath+'CANNON/'+self.reduction_DR+'/'+self.version_cannon+'/diagnostic_plots/Label_distances_'+str(self.obs_date)+'_'+mode+'.png',dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    sp_label_distance_limit = 2*4. # 2 sigma for 4 freedoms\n",
    "    ab_label_distance_limit = 2*5. # 2 sigma for 5 freedoms\n",
    "    if mode=='Sp':\n",
    "        self.cannon['sp_label_distance'] = D\n",
    "        self.cannon['flag_cannon'][D > sp_label_distance_limit] += 1\n",
    "    else:\n",
    "        self.cannon['ld_'+mode+'_abund_cannon'] = D\n",
    "        self.cannon['flag_'+mode+'_abund_cannon'][D > ab_label_distance_limit] += 1\n",
    "        self.cannon['flag_'+mode+'_abund_cannon'][self.cannon['sp_label_distance'] > sp_label_distance_limit] += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_alpha_fe(data_structure,sme_cannon='cannon',alpha_elements=['O','Mg','Si','Ca','Ti']):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    \n",
    "    data_structure : Structure, from which the alpha-process elements come from\n",
    "                     This structure also has to have the keys \n",
    "                     'Alpha_fe_'+sme_cannon and 'e_Alpha_fe_'+sme_cannon\n",
    "    sme_cannon     : 'sme' or 'cannon' which will be use as key within data_structure\n",
    "                     default: 'cannon'\n",
    "    alpha_elements : the array including which alpha-process elements shall be included\n",
    "                     default: ['O','Mg','Si','Ca','Ti']\n",
    "    \n",
    "    OUTPUT:\n",
    "    \n",
    "    None, but the following entries will be filled:\n",
    "    data_structure['Alpha_fe_'+sme_cannon] & data_structure['e_Alpha_fe_'+sme_cannon] \n",
    "    \n",
    "    \"\"\"\n",
    "    for each_sobject_id in range(len(data_structure['sobject_id'])):\n",
    "\n",
    "        combi = np.nansum(np.concatenate([(data_structure[x+'_abund_'+sme_cannon][each_sobject_id]/data_structure['e_'+x+'_abund_'+sme_cannon][each_sobject_id]**2.)[(np.isfinite(data_structure[x+'_abund_'+sme_cannon][each_sobject_id]) & (data_structure['flag_'+x+'_abund_'+sme_cannon][each_sobject_id] == 0))] for x in alpha_elements]))/np.nansum(np.concatenate([1./data_structure['e_'+x+'_abund_'+sme_cannon][each_sobject_id][np.isfinite(data_structure[x+'_abund_'+sme_cannon][each_sobject_id]) & (data_structure['flag_'+x+'_abund_'+sme_cannon][each_sobject_id] == 0)]**2. for x in alpha_elements]))\n",
    "        e_combi = np.concatenate([1./(data_structure['e_'+x+'_abund_'+sme_cannon][each_sobject_id])[np.isfinite(data_structure[x+'_abund_'+sme_cannon][each_sobject_id]) & (data_structure['flag_'+x+'_abund_'+sme_cannon][each_sobject_id] == 0)]**2. for x in alpha_elements])\n",
    "        # If there are enough unflagged values to compute Alpha_fe, go ahead\n",
    "        if len(e_combi) > 0:\n",
    "            e_combi = np.sqrt(1./np.nansum(e_combi))\n",
    "            data_structure['Alpha_fe_'+sme_cannon][each_sobject_id] = combi\n",
    "            data_structure['e_Alpha_fe_'+sme_cannon][each_sobject_id] = e_combi\n",
    "        # Otherwise set values to np.nan\n",
    "        else:\n",
    "            data_structure['Alpha_fe_'+sme_cannon][each_sobject_id] = np.nan\n",
    "            data_structure['e_Alpha_fe_'+sme_cannon][each_sobject_id] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The class 'testset', where we want to put in all important data and routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class testset(object):\n",
    "    \"\"\" \n",
    "    Trainingset is the major class for the important information on the trainingset.\n",
    "    \n",
    "    In the future, this should be executable without the trainingset class\n",
    "    \n",
    "    Initialise with:\n",
    "    obs_date      : Date of observation for the particular testset\n",
    "    wg3_wg4_setup : setup dictionary with all relevant data for IRAF/SME/CANNON\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, obs_date, wg3_wg4_setup):\n",
    "        os.chdir(localFilePath)\n",
    "        self.obs_date           = obs_date\n",
    "        if not hasattr(self,'galah_dict'):\n",
    "            self.galah_dict = create_galah_dict(wg3_wg4_setup)\n",
    "        self.version_cannon     = wg3_wg4_setup['version_cannon']\n",
    "        self.setup_cannon       = wg3_wg4_setup['setup_cannon']\n",
    "        self.nr_ccds_cannon     = wg3_wg4_setup['nr_ccds_cannon']\n",
    "        self.version_reduction  = wg3_wg4_setup['version_reduction']\n",
    "        self.reduction_DR       = wg3_wg4_setup['reduction_DR']\n",
    "        self.version_sme        = wg3_wg4_setup['version_sme']\n",
    "        self.setup_sme          = wg3_wg4_setup['setup_sme']\n",
    "        self.stellar_parameters = wg3_wg4_setup['stellar_parameters']\n",
    "        self.elements           = wg3_wg4_setup['elements']\n",
    "\n",
    "        os.chdir('CANNON/'+self.reduction_DR+'/'+self.version_cannon)\n",
    "        print('You are now working in:'+os.getcwd())\n",
    "        self.SP_data = 'pickle_test/'+self.version_cannon+'_Sp_'+self.setup_cannon+'_'+self.reduction_DR+'_'+str(self.nr_ccds_cannon)+'ccds_'+str(self.obs_date)+'_tags.pickle'\n",
    "        print(self.SP_data)\n",
    "\n",
    "        self.AB_data  = {}\n",
    "        if self.elements!='None':\n",
    "            for each_element in self.elements:\n",
    "                try:\n",
    "                    found_setup = glob.glob('pickle_test/'+self.version_cannon+'_'+each_element+'_'+self.setup_cannon+'_'+self.reduction_DR+'_'+str(self.nr_ccds_cannon)+'ccds_'+str(self.obs_date)+'_tags.pickle')\n",
    "                    self.AB_data[each_element] = found_setup[-1]\n",
    "                    print('Found automatic setup configuration for '+each_element+': '+found_setup[-1])\n",
    "                except:\n",
    "                    print('There is no setup configuration for '+each_element)\n",
    "        self.cannon_trainingset_filename = 'sobject_iraf_'+self.version_cannon+'_trainingset.fits'\n",
    "        self.cannon_filename = 'sobject_iraf_'+self.version_cannon+'.fits'\n",
    "\n",
    "        self.wave               = {}\n",
    "        self.wave['ccd1']       = np.arange(4715.94,4896.00,0.046) # ab lines 4716.3 - 4892.3\n",
    "        self.wave['ccd2']       = np.arange(5650.06,5868.25,0.055) # ab lines 5646.0 - 5867.8\n",
    "        self.wave['ccd3']       = np.arange(6480.52,6733.92,0.064) # ab lines 6481.6 - 6733.4\n",
    "        self.wave['ccd4']       = np.arange(7693.50,7875.55,0.074) # ab lines 7691.2 - 7838.5\n",
    "        self.wave['all']        = np.concatenate((self.wave['ccd1'],self.wave['ccd2'],self.wave['ccd3'],self.wave['ccd4']))\n",
    "        \n",
    "        self.model_flux         = {}\n",
    "        self.model_chi2         = {}\n",
    "        self.spectrum_flux      = {}\n",
    "        self.spectrum_error     = {}\n",
    "        self.masks              = {}\n",
    "\n",
    "    def initiate_cannon_structure(self,nstars):\n",
    "        if not hasattr(self,'cannon'):\n",
    "            print('cannon structure does not exist yet - initialising cannon structure now')\n",
    "            self.cannon = {}\n",
    "            for each_key in self.galah_dict.keys():\n",
    "                self.cannon[each_key] = np.array([self.galah_dict[each_key] for x in range(nstars)])\n",
    "        else: \n",
    "            print('You want to initialise cannon structure, but it already exists!')\n",
    "        \n",
    "    def get_SME(self):\n",
    "        found_SME = glob.glob('trainingset/'+self.version_cannon+'_Sp_*_trainingset.fits')\n",
    "        print('Found automatic setup configuration for SME training set:')\n",
    "        print(found_SME[-1])\n",
    "        self.SME=pyfits.getdata(found_SME[-1])\n",
    "\n",
    "    def get_SP(self):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "        None\n",
    "\n",
    "        OUTPUT:\n",
    "        Adds all important information from the stellar parameter run of the Cannon to the class\n",
    "        \"\"\"\n",
    "        params, e_params, covs, chi2, sobject_id, chi2good, chi2each = read_model_pickle(self.SP_data)\n",
    "\n",
    "        if not hasattr(self,'cannon'):\n",
    "            self.initiate_cannon_structure(len(sobject_id))\n",
    "\n",
    "        self.cannon['sobject_id'] = np.array([int(sobject_id[x]) for x in range(len(sobject_id))])\n",
    "\n",
    "        for index, each_param in enumerate(self.stellar_parameters):\n",
    "            self.cannon[each_param+'_cannon'] = params[:,index]\n",
    "            self.cannon['e_'+each_param+'_cannon'] = e_params[:,index]\n",
    "        self.cannon['chi2_cannon'] = chi2good\n",
    "\n",
    "        # Neglect those with chi2 above high and below low limit (expectation is >1.0)\n",
    "        # Chosen because of 5 times too high chi2\n",
    "        high_chi2_limit = 5.\n",
    "        # Chosen because of strange overdensity below 0.4\n",
    "        low_chi2_limit = 0.4 \n",
    "        self.cannon['flag_cannon'][self.cannon['chi2_cannon'] > high_chi2_limit] += 2 # chi2 too high\n",
    "        self.cannon['flag_cannon'][self.cannon['chi2_cannon'] <  low_chi2_limit] += 2 # chi2 too low\n",
    "\n",
    "\n",
    "        #     red_flag\n",
    "        #   0 for no flags\n",
    "        # + 1 for bad wavelength solution in ccd_1, \n",
    "        # + 2 for bad wavelength solution in ccd_2, \n",
    "        # + 4 for bad wavelength solution in ccd_3, \n",
    "        # + 8 for bad wavelength solution in ccd_4,\n",
    "        # +16 for molecfit fail in ccd_3\n",
    "        # +32 for molecfit fail in ccd_4\n",
    "        # +64 for twilight flat\n",
    "\n",
    "        # Reduction fail\n",
    "        self.cannon['flag_cannon'][self.cannon['red_flag'] > 0] += 4\n",
    "\n",
    "        # Estimate label distance for stellar parameters\n",
    "        label_distance(self, mode='Sp')  \n",
    "\n",
    "    def get_IRAF(self):\n",
    "        if hasattr(self,'cannon'):\n",
    "            print('Reading in IRAF reduction version: '+self.version_reduction)\n",
    "            iraf = pyfits.getdata(localFilePath+'/DATA/'+self.version_reduction+'.fits',ext=1)\n",
    "            for cannon_index,each_sobject_id in enumerate(self.cannon['sobject_id']):\n",
    "                iraf_equivalent = np.where(each_sobject_id == iraf['sobject_id'])[0]\n",
    "                if len(iraf_equivalent) > 0:\n",
    "                    for each_iraf_label in [\n",
    "                            'galah_id','field_id','ra','dec','ebv',\n",
    "                            'snr_c1_iraf','snr_c2_iraf','snr_c3_iraf','snr_c4_iraf','red_flag',\n",
    "                            'teff_guess','logg_guess','feh_guess','rv_guess','e_rv_guess','rv_guess_shift','flag_guess'\n",
    "                            ]:\n",
    "                        self.cannon[each_iraf_label][cannon_index] = iraf[each_iraf_label][iraf_equivalent[0]]\n",
    "                else:\n",
    "                    print('Something went wrong here, because there is no IRAF entry for: '+str(each_sobject_id))\n",
    "\n",
    "            # Once snr_c2_iraf is filled, we can update the stellar parameter errors\n",
    "            precision = np.load(localFilePath+'CANNON/'+self.reduction_DR+'/'+self.version_cannon+'/fits_files/'+self.version_cannon+'_'+self.setup_cannon+'_'+self.reduction_DR+'_'+str(self.nr_ccds_cannon)+'ccds_precision.npy').item()\n",
    "            def precision_snr(x, floor, amp, damp):\n",
    "                return abs(floor) + amp * np.exp( - x *damp)\n",
    "            for each_label in self.stellar_parameters:\n",
    "                self.cannon['e_'+each_label+'_cannon'] = (\n",
    "                        (self.cannon['e_'+each_label+'_cannon'])**2. + \n",
    "                        (precision_snr(self.cannon['snr_c2_iraf'], *precision['cannon_precision_function'][each_label]))**2.\n",
    "                    )**0.5\n",
    "\n",
    "            self.cannon['flag_cannon'][(self.cannon['ra'] == 0) & (self.cannon['dec'] == 90)] += 2 # deselected special pointings\n",
    "\n",
    "        else:\n",
    "            print('You did not call testset.get_SP() yet, which initialises the CANNON attribute to save the abundances in!')\n",
    "\n",
    "    def get_tSNE(self):\n",
    "\n",
    "        t = table.Table()\n",
    "        tsne = t.read(localFilePath+'DATA/dr52_class_joined.fits')\n",
    "\n",
    "        \"\"\"\n",
    "        Individual classifications are:\n",
    "\n",
    "        The ones we will use:\n",
    "\n",
    "        binary\n",
    "        triple\n",
    "\n",
    "        HaHb emission\n",
    "\n",
    "        problematic\n",
    "        problematic - ccd2 continuum\n",
    "        problematic - ccd3 continuum\n",
    "\n",
    "        problematic - osc. cont.\n",
    "        problematic - ccd4\n",
    "        problematic - ccd4 oversubtraction\n",
    "        problematic - neg. flux\n",
    "        problematic - negative flux\n",
    "        problematic - ccd1\n",
    "\n",
    "        problematic - ccd3 spikes\n",
    "        problematic - ccd4 spikes\n",
    "        problematic - ccd4 strong spike\n",
    "\n",
    "        The ones we wont use\n",
    "\n",
    "        mol. abs. bands\n",
    "        hot stars\n",
    "        CMP giants\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        tsne_flag = {}\n",
    "\n",
    "        tsne_flag['binary'] = 8\n",
    "        tsne_flag['trippe'] = 8\n",
    "\n",
    "        tsne_flag['problematic - neg. flux'] = 16\n",
    "        tsne_flag['problematic - negative flux'] = 16\n",
    "\n",
    "        tsne_flag['problematic - osc. cont.'] = 32\n",
    "        tsne_flag['problematic - ccd2 continuum'] = 32\n",
    "        tsne_flag['problematic - ccd3 continuum'] = 32\n",
    "\n",
    "        tsne_flag['problematic'] = 32\n",
    "        tsne_flag['problematic - ccd1'] = 32\n",
    "        tsne_flag['problematic - ccd3 spikes'] = 32\n",
    "\n",
    "        tsne_flag['problematic - ccd4'] = 64\n",
    "        tsne_flag['problematic - ccd4 oversubtraction'] = 64\n",
    "        tsne_flag['problematic - ccd4 spikes'] = 64\n",
    "        tsne_flag['problematic - ccd4 strong spike'] = 64\n",
    "\n",
    "        tsne_flag['HaHb emission'] = 128\n",
    "\n",
    "        for each_tsne_flag in tsne_flag.keys():\n",
    "\n",
    "            run1 = tsne['sobject_id'][tsne['class_norm']     ==each_tsne_flag]\n",
    "            run2 = tsne['sobject_id'][tsne['noIR_class_norm']==each_tsne_flag]\n",
    "\n",
    "            exclude_tsne_flagged = []\n",
    "            if (len(run1) > 0) & (len(run2) > 0):\n",
    "                exclude_tsne_flagged = np.unique(\n",
    "                                       np.concatenate((\n",
    "                                           tsne['sobject_id'][tsne['class_norm']     ==each_tsne_flag],\n",
    "                                           tsne['sobject_id'][tsne['noIR_class_norm']==each_tsne_flag]\n",
    "                                           ))\n",
    "                                       )\n",
    "            elif len(run1) > 0:\n",
    "                exclude_tsne_flagged = run1\n",
    "            elif len(run2) > 0:\n",
    "                exclude_tsne_flagged = run2\n",
    "\n",
    "            for each_sobject_id_to_glag in exclude_tsne_flagged:\n",
    "                found = np.where(each_sobject_id_to_glag == self.cannon['sobject_id'])[0]\n",
    "                if len(found) > 0:\n",
    "                    self.cannon['flag_cannon'][found[0]] += tsne_flag[each_tsne_flag]\n",
    "                    \n",
    "    def get_AB(self):\n",
    "        \"\"\"\n",
    "        Routine to get all abundance information\n",
    "\n",
    "        INPUT:\n",
    "        None, but class attribute 'cannon' must exist!\n",
    "\n",
    "        OUTPUT:\n",
    "        Adds all important information from the element abundance run of the Cannon to the class\n",
    "        \"\"\"\n",
    "        if hasattr(self,'cannon'):\n",
    "            for each_element in self.elements:\n",
    "                try:\n",
    "\n",
    "                    # Get line masks\n",
    "                    self.masks[each_element] = np.loadtxt(localFilePath+'CANNON/'+self.reduction_DR+'/masks_4ccds/'+self.reduction_DR+'_'+each_element+'.txt',usecols=(5,),unpack=1)\n",
    "\n",
    "                    # Get labels from Cannon\n",
    "                    params, e_params, covs, chi2, sobject_ids, chi2good, chi2each = read_model_pickle(self.AB_data[each_element])\n",
    "                    self.cannon[each_element+'_abund_cannon'] = params[:,-1]\n",
    "\n",
    "                    # Read in test set spectral data\n",
    "                    testdataname=localFilePath+'CANNON/'+self.reduction_DR+'/pickle_'+self.reduction_DR+'_4ccds/'+self.reduction_DR+'_4ccds_'+str(self.obs_date)+'.pickle'\n",
    "                    door=open(testdataname,'r')\n",
    "                    testdataall, ids, galah_name = pickle.load(door)\n",
    "                    door.close()\n",
    "\n",
    "                    # Read in model for element\n",
    "                    use_model = open(localFilePath+'CANNON/'+self.reduction_DR+'/'+self.version_cannon+'/'+self.version_cannon+'_'+each_element+'_'+self.setup_cannon+'_model.pickle')\n",
    "                    dataall, metaall, labels_train, offsets, coeffs, covs, scatters, chis, chisqs = pickle.load(use_model)\n",
    "                    use_model.close()\n",
    "\n",
    "                    # We will now estimate the line depths based on the model and spectrum flux\n",
    "                    model_flux     = []\n",
    "                    spectrum_flux  = []\n",
    "                    spectrum_error = []\n",
    "\n",
    "                    for i in range(len(self.cannon['sobject_id'])):\n",
    "\n",
    "                        features_data = np.hstack((1, params[i] - offsets))\n",
    "                        newfeatures_data = np.array([np.outer(params[i]-offsets,params[i]-offsets)[np.triu_indices(len(params[i]))]])\n",
    "                        features_data_final = np.hstack((features_data,newfeatures_data.flatten()))\n",
    "                        model_gen2 = np.dot(coeffs,features_data_final.T)\n",
    "\n",
    "                        model_flux.append(model_gen2)\n",
    "                        spectrum_flux.append(testdataall[:,i,1])\n",
    "                        spectrum_error.append(testdataall[:,i,2])\n",
    "\n",
    "                    self.model_flux[each_element] = np.array(model_flux)\n",
    "                    self.model_chi2[each_element] = chi2each\n",
    "                    self.spectrum_flux[each_element] = np.array(spectrum_flux)\n",
    "                    self.spectrum_error[each_element] = np.array(spectrum_error)\n",
    "\n",
    "                    # Load SNR-depending precision fucntion for X_abund_cannon to increase e_X_abund_cannon\n",
    "                    precision = np.load(localFilePath+'CANNON/'+self.reduction_DR+'/'+self.version_cannon+'/fits_files/'+self.version_cannon+'_'+self.setup_cannon+'_'+self.reduction_DR+'_'+str(self.nr_ccds_cannon)+'ccds_precision.npy').item()\n",
    "                    def precision_snr(x, floor, amp, damp):\n",
    "                        return abs(floor) + amp * np.exp( - x *damp)\n",
    "                    self.cannon['e_'+each_element+'_abund_cannon'] = (\n",
    "                            (e_params[:,-1])**2. + \n",
    "                            (precision_snr(self.cannon['snr_c2_iraf'], *precision['cannon_precision_function'][each_element]))**2.\n",
    "                        )**0.5\n",
    "\n",
    "                    # Estimate which lines are significant detections analogous to SME approach\n",
    "                    get_detections(self, mode=each_element)\n",
    "\n",
    "                    # Estimate label distance from 10 closest training set stars for element\n",
    "                    label_distance(self, mode=each_element)\n",
    "\n",
    "                except:\n",
    "                    print('Element '+each_element+' not available!')\n",
    "                    self.cannon[each_element+'_abund_cannon'][:] = np.nan\n",
    "                    self.cannon['e_'+each_element+'_abund_cannon'][:] = np.nan\n",
    "                    self.cannon['flag_'+each_element+'_abund_cannon'][:] = -1\n",
    "                    self.cannon['ld_'+each_element+'_abund_cannon'][:] = np.nan\n",
    "                    self.cannon['depth_'+each_element+'_abund_cannon'][:] = np.nan\n",
    "                    self.cannon['sn_'+each_element+'_abund_cannon'][:] = np.nan\n",
    "                    self.cannon['chi2_'+each_element+'_abund_cannon'][:] = np.nan\n",
    "\n",
    "            compute_alpha_fe(self.cannon,sme_cannon='cannon',alpha_elements=['O','Mg','Si','Ca','Ti'])\n",
    "        else:\n",
    "            print('You did not call testset.get_SP() and testset.get_IRAF() yet, which initialises the CANNON attribute to save the abundances in!')\n",
    "\n",
    "\n",
    "    def export_fits(self):\n",
    "        export_fits(\n",
    "            self.cannon, \n",
    "            filename=localFilePath+'CANNON/'+self.reduction_DR+'/'+self.version_cannon+'/fits_files/'+self.version_cannon+'_'+self.setup_cannon+'_'+self.reduction_DR+'_'+str(self.nr_ccds_cannon)+'ccds_'+str(self.obs_date)\n",
    "        )\n",
    "\n",
    "    def plot_HRD(self, savefig=False):\n",
    "        plot_HRD(self.cannon, sme_cannon='cannon', savefig=savefig)\n",
    "        \n",
    "    def plot_alpha(self, savefig=False):\n",
    "        plot_alpha(self.cannon, sme_cannon='cannon', savefig=savefig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running the Cannon in IPYNB mode\n",
      "Cannon3.0.1 dr5.2 131119\n"
     ]
    }
   ],
   "source": [
    "if sys.argv[1] == '-f':\n",
    "    print('You are running the Cannon in IPYNB mode')\n",
    "    version_cannon = 'Cannon3.0.1'\n",
    "    reduction_DR   = 'dr5.2'\n",
    "    obs_date       = 131119\n",
    "    print(version_cannon,reduction_DR,obs_date)\n",
    "else:\n",
    "    print('You are running the Cannon in PY mode')\n",
    "    version_cannon = sys.argv[1]\n",
    "    reduction_DR   = sys.argv[2]\n",
    "    obs_date       = sys.argv[3]\n",
    "\n",
    "wg3_wg4_setup = dict(\n",
    "    version_reduction  = 'sobject_iraf_52_171009',\n",
    "    reduction_DR       = reduction_DR,\n",
    "    version_sme        = 'SME360_DR2_SVN331',\n",
    "    setup_sme          = 'DR2',\n",
    "    version_cannon     = version_cannon,\n",
    "    setup_cannon       = 'SMEmasks_it1',\n",
    "    nr_ccds_cannon     = 4,\n",
    "    stellar_parameters = ['Teff', 'Logg', 'Feh', 'Vmic', 'Vsini', 'Ak'],\n",
    "    elements           = [\n",
    "                         'Li',  'C',  'O', 'Na', 'Mg', 'Al', 'Si',  'K', 'Ca', 'Sc', \n",
    "                         'Ti',  'V', 'Cr', 'Mn', 'Co', 'Ni', 'Cu', 'Zn', 'Rb', 'Sr',\n",
    "                          'Y', 'Zr', 'Mo', 'Ru', 'Ba', 'La', 'Ce', 'Nd', 'Sm', 'Eu'\n",
    "                         ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create the class for the testset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are now working in:/shared-storage/buder/svn-repos/trunk/GALAH/CANNON/dr5.2/Cannon3.0.1\n",
      "pickle_test/Cannon3.0.1_Sp_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Li: pickle_test/Cannon3.0.1_Li_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for C: pickle_test/Cannon3.0.1_C_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for O: pickle_test/Cannon3.0.1_O_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Na: pickle_test/Cannon3.0.1_Na_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Mg: pickle_test/Cannon3.0.1_Mg_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Al: pickle_test/Cannon3.0.1_Al_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Si: pickle_test/Cannon3.0.1_Si_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for K: pickle_test/Cannon3.0.1_K_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Ca: pickle_test/Cannon3.0.1_Ca_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Sc: pickle_test/Cannon3.0.1_Sc_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Ti: pickle_test/Cannon3.0.1_Ti_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for V: pickle_test/Cannon3.0.1_V_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Cr: pickle_test/Cannon3.0.1_Cr_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Mn: pickle_test/Cannon3.0.1_Mn_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Co: pickle_test/Cannon3.0.1_Co_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Ni: pickle_test/Cannon3.0.1_Ni_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Cu: pickle_test/Cannon3.0.1_Cu_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Zn: pickle_test/Cannon3.0.1_Zn_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Rb: pickle_test/Cannon3.0.1_Rb_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Sr: pickle_test/Cannon3.0.1_Sr_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Y: pickle_test/Cannon3.0.1_Y_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Zr: pickle_test/Cannon3.0.1_Zr_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Mo: pickle_test/Cannon3.0.1_Mo_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Ru: pickle_test/Cannon3.0.1_Ru_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Ba: pickle_test/Cannon3.0.1_Ba_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for La: pickle_test/Cannon3.0.1_La_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Ce: pickle_test/Cannon3.0.1_Ce_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Nd: pickle_test/Cannon3.0.1_Nd_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Sm: pickle_test/Cannon3.0.1_Sm_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Found automatic setup configuration for Eu: pickle_test/Cannon3.0.1_Eu_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Sp_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "cannon structure does not exist yet - initialising cannon structure now\n",
      "Reading in IRAF reduction version: sobject_iraf_52_171009\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Li_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_C_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_O_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Na_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Mg_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Al_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Si_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_K_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Ca_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Sc_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Ti_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_V_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Cr_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Mn_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Co_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Ni_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Cu_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Zn_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Rb_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Sr_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Y_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Zr_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Mo_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Ru_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Ba_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_La_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Ce_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Nd_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Sm_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n",
      "Will read pickle file: pickle_test/Cannon3.0.1_Eu_SMEmasks_it1_dr5.2_4ccds_131119_tags.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buder/.local/lib/python2.7/site-packages/ipykernel/__main__.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished collecting 131119\n"
     ]
    }
   ],
   "source": [
    "if obs_date == 'all':\n",
    "    obs_date_todo = glob.glob(localFilePath+'CANNON/'+reduction_DR+'/'+version_cannon+'/pickle_test/*Sp*.pickle')\n",
    "    for each_obs_date in obs_date_todo:\n",
    "        try:\n",
    "            each_testset = testset(int(each_obs_date[-18:-12]), wg3_wg4_setup)\n",
    "            each_testset.get_SP()\n",
    "            each_testset.get_IRAF()\n",
    "            each_testset.get_tSNE()\n",
    "            each_testset.get_AB()\n",
    "            each_testset.export_fits()\n",
    "        except:\n",
    "            pass\n",
    "else:\n",
    "    if sys.argv[1] == '-f':\n",
    "        each_testset = testset(obs_date, wg3_wg4_setup)\n",
    "    else:\n",
    "        each_testset = testset(int(obs_date[-13:-7]), wg3_wg4_setup)\n",
    "    each_testset.get_SP()\n",
    "    each_testset.get_IRAF()\n",
    "    each_testset.get_tSNE()\n",
    "    each_testset.get_AB()\n",
    "    each_testset.export_fits()\n",
    "    print('Finished collecting '+str(obs_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert IPYNB to PY\n",
    "\n",
    "os.chdir('/shared-storage/buder/svn-repos/trunk/GALAH/TheGALAHCannon/')\n",
    "\n",
    "convert_command = 'ipython nbconvert --to script Cannon_collect_test.ipynb'\n",
    "os.system(convert_command)\n",
    "\n",
    "os.chdir('/shared-storage/buder/svn-repos/trunk/GALAH/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
