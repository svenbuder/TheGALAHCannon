{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cannon_collect_all\n",
    "\n",
    "This routine is collecting the data from the training set run\n",
    "\n",
    "Author(s): Sven Buder\n",
    "\n",
    "History:\n",
    "171020: SB Create file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:PyMultiNest not imported.  MultiNest fits will not work.\n",
      "WARNING:root:You have a wrong/corrupted/outdated Dartmouth triangulation! Delete /shared-storage/buder/Github/isochrones/ISOCHRONE_DATA/dartmouth.tri and try re-importing to download afresh.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dartmouth stellar model data (should happen only once)...\n",
      "Could not load isochrones package\n"
     ]
    }
   ],
   "source": [
    "# Compatibility with Python 3\n",
    "from __future__ import (absolute_import, division, print_function)\n",
    "\n",
    "try:\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format='retina'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Basic packages\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "# Packages to work with FITS and (IDL) SME.out files\n",
    "import astropy.io.fits as pyfits\n",
    "import astropy.table as table\n",
    "from scipy.io.idl import readsav\n",
    "\n",
    "# Matplotlib and associated packages for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "params = {\n",
    "    'font.family'        : 'lmodern',\n",
    "    'font.size'          : 17,\n",
    "    'axes.labelsize'     : 20,\n",
    "    'ytick.labelsize'    : 16,\n",
    "    'xtick.labelsize'    : 16,\n",
    "    'legend.fontsize'    : 20,\n",
    "    'text.usetex'        : True, \n",
    "    'text.latex.preamble': [r'\\usepackage{upgreek}', r'\\usepackage{amsmath}'],\n",
    "    }   \n",
    "plt.rcParams.update(params)\n",
    "\n",
    "_parula_data = [[0.2081, 0.1663, 0.5292], \n",
    "                [0.2116238095, 0.1897809524, 0.5776761905], \n",
    "                [0.212252381, 0.2137714286, 0.6269714286], \n",
    "                [0.2081, 0.2386, 0.6770857143], \n",
    "                [0.1959047619, 0.2644571429, 0.7279], \n",
    "                [0.1707285714, 0.2919380952, 0.779247619], \n",
    "                [0.1252714286, 0.3242428571, 0.8302714286], \n",
    "                [0.0591333333, 0.3598333333, 0.8683333333], \n",
    "                [0.0116952381, 0.3875095238, 0.8819571429], \n",
    "                [0.0059571429, 0.4086142857, 0.8828428571], \n",
    "                [0.0165142857, 0.4266, 0.8786333333], \n",
    "                [0.032852381, 0.4430428571, 0.8719571429], \n",
    "                [0.0498142857, 0.4585714286, 0.8640571429], \n",
    "                [0.0629333333, 0.4736904762, 0.8554380952], \n",
    "                [0.0722666667, 0.4886666667, 0.8467], \n",
    "                [0.0779428571, 0.5039857143, 0.8383714286], \n",
    "                [0.079347619, 0.5200238095, 0.8311809524], \n",
    "                [0.0749428571, 0.5375428571, 0.8262714286], \n",
    "                [0.0640571429, 0.5569857143, 0.8239571429], \n",
    "                [0.0487714286, 0.5772238095, 0.8228285714], \n",
    "                [0.0343428571, 0.5965809524, 0.819852381], \n",
    "                [0.0265, 0.6137, 0.8135], \n",
    "                [0.0238904762, 0.6286619048, 0.8037619048], \n",
    "                [0.0230904762, 0.6417857143, 0.7912666667], \n",
    "                [0.0227714286, 0.6534857143, 0.7767571429], \n",
    "                [0.0266619048, 0.6641952381, 0.7607190476], \n",
    "                [0.0383714286, 0.6742714286, 0.743552381], \n",
    "                [0.0589714286, 0.6837571429, 0.7253857143], \n",
    "                [0.0843, 0.6928333333, 0.7061666667], \n",
    "                [0.1132952381, 0.7015, 0.6858571429], \n",
    "                [0.1452714286, 0.7097571429, 0.6646285714], \n",
    "                [0.1801333333, 0.7176571429, 0.6424333333], \n",
    "                [0.2178285714, 0.7250428571, 0.6192619048], \n",
    "                [0.2586428571, 0.7317142857, 0.5954285714], \n",
    "                [0.3021714286, 0.7376047619, 0.5711857143], \n",
    "                [0.3481666667, 0.7424333333, 0.5472666667], \n",
    "                [0.3952571429, 0.7459, 0.5244428571], \n",
    "                [0.4420095238, 0.7480809524, 0.5033142857], \n",
    "                [0.4871238095, 0.7490619048, 0.4839761905], \n",
    "                [0.5300285714, 0.7491142857, 0.4661142857], \n",
    "                [0.5708571429, 0.7485190476, 0.4493904762],\n",
    "                [0.609852381, 0.7473142857, 0.4336857143], \n",
    "                [0.6473, 0.7456, 0.4188], \n",
    "                [0.6834190476, 0.7434761905, 0.4044333333], \n",
    "                [0.7184095238, 0.7411333333, 0.3904761905], \n",
    "                [0.7524857143, 0.7384, 0.3768142857], \n",
    "                [0.7858428571, 0.7355666667, 0.3632714286], \n",
    "                [0.8185047619, 0.7327333333, 0.3497904762], \n",
    "                [0.8506571429, 0.7299, 0.3360285714], \n",
    "                [0.8824333333, 0.7274333333, 0.3217], \n",
    "                [0.9139333333, 0.7257857143, 0.3062761905], \n",
    "                [0.9449571429, 0.7261142857, 0.2886428571], \n",
    "                [0.9738952381, 0.7313952381, 0.266647619], \n",
    "                [0.9937714286, 0.7454571429, 0.240347619], \n",
    "                [0.9990428571, 0.7653142857, 0.2164142857], \n",
    "                [0.9955333333, 0.7860571429, 0.196652381], \n",
    "                [0.988, 0.8066, 0.1793666667], \n",
    "                [0.9788571429, 0.8271428571, 0.1633142857], \n",
    "                [0.9697, 0.8481380952, 0.147452381], \n",
    "                [0.9625857143, 0.8705142857, 0.1309], \n",
    "                [0.9588714286, 0.8949, 0.1132428571], \n",
    "                [0.9598238095, 0.9218333333, 0.0948380952], \n",
    "                [0.9661, 0.9514428571, 0.0755333333], \n",
    "                [0.9763, 0.9831, 0.0538]]\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "parula = ListedColormap(_parula_data, name='parula')\n",
    "parula_zero = _parula_data[0]\n",
    "\n",
    "# Import isochrone package to plot isochrones\n",
    "try:\n",
    "    from isochrones.dartmouth import Dartmouth_Isochrone\n",
    "    dart = Dartmouth_Isochrone()\n",
    "    from isochrones import Isochrone\n",
    "except:\n",
    "    print('Could not load isochrones package')\n",
    "    \n",
    "# Package to save multiple PDF pages in one PDF\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /shared-storage/buder/svn-repos/trunk/GALAH\n"
     ]
    }
   ],
   "source": [
    "# Change Work directory (if Sven's computer)\n",
    "try:\n",
    "    localFilePath = '/shared-storage/buder/svn-repos/trunk/GALAH/'\n",
    "    os.chdir(localFilePath)\n",
    "    print('Current working directory: '+os.getcwd())\n",
    "except:\n",
    "    print('Could not change Path to '+localFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tex_dict = dict(\n",
    "    Teff     = r'$T_\\mathrm{eff}~\\mathrm{[K]}$',\n",
    "    Logg     = r'$\\log g~\\mathrm{[dex]}$',\n",
    "    Feh      = r'$\\mathrm{[Fe/H]~[dex]}$',\n",
    "    Vmic     = r'$v_\\mathrm{mic}~\\mathrm{[km/s]}$',\n",
    "    Vsini    = r'$v_\\mathrm{broad}~\\mathrm{[km/s]}$',\n",
    "    Alpha_fe = r'$\\mathrm{[\\alpha/Fe]~[dex]}$',\n",
    "    XFe      = r'$\\mathrm{[X/Fe]~[dex]}$',\n",
    "    SNR      = r'$\\mathrm{Green~Channel~S/N}$',\n",
    "    Li        = r'$\\mathrm{[Li/Fe]~[dex]}$',\n",
    "    C        = r'$\\mathrm{[C/Fe]~[dex]}$',\n",
    "    O        = r'$\\mathrm{[O/Fe]~[dex]}$',\n",
    "    Na       = r'$\\mathrm{[Na/Fe]~[dex]}$',\n",
    "    Mg       = r'$\\mathrm{[Mg/Fe]~[dex]}$',\n",
    "    Al       = r'$\\mathrm{[Al/Fe]~[dex]}$',\n",
    "    Si       = r'$\\mathrm{[Si/Fe]~[dex]}$',\n",
    "    K        = r'$\\mathrm{[K/Fe]~[dex]}$',\n",
    "    Ca       = r'$\\mathrm{[Ca/Fe]~[dex]}$',\n",
    "    Sc       = r'$\\mathrm{[Sc/Fe]~[dex]}$',\n",
    "    Ti       = r'$\\mathrm{[Ti/Fe]~[dex]}$',\n",
    "    V        = r'$\\mathrm{[V/Fe]~[dex]}$',\n",
    "    Cr       = r'$\\mathrm{[Cr/Fe]~[dex]}$',\n",
    "    Mn       = r'$\\mathrm{[Mn/Fe]~[dex]}$',\n",
    "    Co       = r'$\\mathrm{[Co/Fe]~[dex]}$',\n",
    "    Ni       = r'$\\mathrm{[Ni/Fe]~[dex]}$',\n",
    "    Zn       = r'$\\mathrm{[Zn/Fe]~[dex]}$',\n",
    "    Rb       = r'$\\mathrm{[Rb/Fe]~[dex]}$',\n",
    "    Sr       = r'$\\mathrm{[Sr/Fe]~[dex]}$',\n",
    "    Y        = r'$\\mathrm{[Y/Fe]~[dex]}$',\n",
    "    Zr       = r'$\\mathrm{[Zr/Fe]~[dex]}$',\n",
    "    Mo       = r'$\\mathrm{[Mo/Fe]~[dex]}$',\n",
    "    Ru       = r'$\\mathrm{[Ru/Fe]~[dex]}$',\n",
    "    Ba       = r'$\\mathrm{[Ba/Fe]~[dex]}$',\n",
    "    La       = r'$\\mathrm{[La/Fe]~[dex]}$',\n",
    "    Ce       = r'$\\mathrm{[Ce/Fe]~[dex]}$',\n",
    "    Nd       = r'$\\mathrm{[Nd/Fe]~[dex]}$',\n",
    "    Sm       = r'$\\mathrm{[Sm/Fe]~[dex]}$',\n",
    "    Eu       = r'$\\mathrm{[Eu/Fe]~[dex]}$'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def export_fits(dictionary, filename):\n",
    "    \"\"\"\n",
    "    This is a function to export a datastructure, nomatter where you currently are\n",
    "    \n",
    "    INPUT:\n",
    "    dictionary : dictionary data_structure\n",
    "    filename   : how should the file be called? Best: include the full path\n",
    "    \n",
    "    OUTPUT:\n",
    "    FITS file saved as filename\n",
    "    \"\"\"\n",
    "    \n",
    "    t = table.Table()\n",
    "    for each_key in dictionary.keys():\n",
    "        t.add_column(table.Column(name=each_key, data=dictionary[each_key]))\n",
    "    t.write(filename+'.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_galah_dict(wg3_wg4_setup):\n",
    "    galah_dict = dict(\n",
    "\n",
    "        # General\n",
    "        sobject_id  = 112233000001234,\n",
    "        galah_id    = 1234567,\n",
    "        field_id    = 0,\n",
    "        ra          = 0.0,\n",
    "        dec         = 0.0,\n",
    "        ebv         = 0.0,\n",
    "        snr_c1_iraf = 0.0,\n",
    "        snr_c2_iraf = 0.0,\n",
    "        snr_c3_iraf = 0.0,\n",
    "        snr_c4_iraf = 0.0,\n",
    "        teff_guess  = 0.0,\n",
    "        logg_guess  = 0.0,\n",
    "        feh_guess   = 0.0,\n",
    "        rv_guess    = 0.0,\n",
    "        e_rv_guess  = 0.0,\n",
    "        flag_guess  = 0.0,\n",
    "\n",
    "        # General SME\n",
    "        trained_on     = 0,\n",
    "        rv_sme         = 0.0,\n",
    "        e_rv_sme       = 0.0,\n",
    "        Alpha_fe_sme   = 0.0,\n",
    "        e_Alpha_fe_sme = 0.0,\n",
    "\n",
    "        # General Cannon\n",
    "        chi2_cannon       = 0.0,\n",
    "        sp_label_distance = 0.0,\n",
    "        flag_cannon       = 0,\n",
    "        Alpha_fe_cannon   = 0.0,\n",
    "        e_Alpha_fe_cannon = 0.0\n",
    "        )\n",
    "\n",
    "    for each_method in ['sme','cannon']:\n",
    "        for each_sp in wg3_wg4_setup['stellar_parameters']:\n",
    "            galah_dict[each_sp+'_'+each_method] = 0.0\n",
    "            galah_dict['e_'+each_sp+'_'+each_method] = 0.0\n",
    "\n",
    "        for each_elem in wg3_wg4_setup['elements']:\n",
    "            galah_dict[each_elem+'_abund_'+each_method]         = 0.0\n",
    "            galah_dict['e_'+each_elem+'_abund_'+each_method]    = 0.0\n",
    "            galah_dict['ld_'+each_elem+'_abund_'+each_method]   = 0.0\n",
    "            galah_dict['flag_'+each_elem+'_abund_'+each_method] = 0\n",
    "            \n",
    "    return galah_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_model_pickle(filename):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    \n",
    "    filename : pickle filename path\n",
    "    \n",
    "    OUTPUT:\n",
    "    pickle file structure\n",
    "    \n",
    "    \"\"\"\n",
    "    door = open(filename,'r')\n",
    "    print('Will read pickle file: '+filename)\n",
    "    model_data = pickle.load(door)\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_isochrones(ax=plt.gca):\n",
    "    try:\n",
    "        for each in np.arange(1,11):\n",
    "            dart_iso = Isochrone.isochrone(dart,age=np.log10(each*10**9.),feh=0.0,dm=0.001)\n",
    "            dart_iso1_1, = ax.plot(dart_iso.Teff,dart_iso.logg,color='grey',lw=0.5,alpha=0.75)\n",
    "        dart_iso = Isochrone.isochrone(dart,age=np.log10(10*10**9.),feh=-1.0,dm=0.001)\n",
    "        dart_iso1_2, = ax.plot(dart_iso.Teff,dart_iso.logg,color='grey',ls='dotted',lw=2,alpha=0.75)\n",
    "        dart_iso = Isochrone.isochrone(dart,age=np.log10(10*10**9.),feh=+0.5,dm=0.001)\n",
    "        ax.plot(dart_iso.Teff,dart_iso.logg,color='grey',ls='dotted',lw=2,alpha=0.75)\n",
    "    except:\n",
    "        print('Could not plot isochrones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_contours(X, Y, xlim, ylim, ax=plt.gca):\n",
    "    XY_good = (np.isfinite(X) & np.isfinite(Y))\n",
    "    H, xedges, yedges = np.histogram2d(Y[XY_good],X[XY_good], range=[(ylim[-1],ylim[0]), (xlim[-1],xlim[0])], bins=(100, 100))\n",
    "    extent = [yedges[0], yedges[-1], xedges[0], xedges[-1]]\n",
    "    plt.subplots_adjust(bottom=0.15, left=0.15)\n",
    "    levels = (5,10,30)\n",
    "    cset = ax.contour(H, levels, colors=['black','black','black'],linewidths=(0.25, 0.5,1.),extent=extent,zorder=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_HRD(dictionary, sme_cannon='cannon', savefig=False, xlim=(7900,3600), ylim=(5.,0.)):\n",
    "    HRD_kwargs = dict(cmap=parula, s=20, lw=0.1, rasterized=True)\n",
    "    f, (ax1,ax2) = plt.subplots(1,2,figsize=(2*6.4, 4.8))\n",
    "\n",
    "    s1 = ax1.scatter(dictionary['Teff_'+sme_cannon],dictionary['Logg_'+sme_cannon],c=dictionary['Feh_'+sme_cannon], vmin=-2.5, vmax=0.5,**HRD_kwargs)\n",
    "    ax1.set_xlabel(tex_dict['Teff'])\n",
    "    ax1.set_ylabel(tex_dict['Logg'])\n",
    "    c1 = plt.colorbar(s1, ax=ax1)\n",
    "    c1.set_label(tex_dict['Feh'])\n",
    "    ax1.set_xlim(xlim)\n",
    "    ax1.set_ylim(ylim)\n",
    "    \n",
    "    s2 = ax2.scatter(dictionary['Teff_'+sme_cannon],dictionary['Logg_'+sme_cannon],c=dictionary['snr_c2_iraf'], vmin=25, vmax=160,**HRD_kwargs)\n",
    "    ax2.set_xlabel(tex_dict['Teff'])\n",
    "    ax2.set_ylabel(tex_dict['Logg'])\n",
    "    c2 = plt.colorbar(s2, ax=ax2)\n",
    "    c2.set_label(tex_dict['SNR'])\n",
    "    ax2.set_xlim(xlim)\n",
    "    ax2.set_ylim(ylim)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if savefig!=False:\n",
    "        plt.savefig(savefig+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ab_scatter(X, Y, ax=plt.gca, **kwargs):\n",
    "    \"\"\"\n",
    "    This function gives back a scatter plot\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    c = kwargs.get('c',parula_zero)\n",
    "    s = kwargs.get('s',2)\n",
    "    s1 = ax.scatter(X,Y,c=c,s=s,alpha=0.5,rasterized=True)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "def ab_dens2d(X, Y, ax=plt.gca, min_per_bin=5, zeroslines=True, interimlines=True, colorbar=True, **kwargs):\n",
    "    \"\"\"\n",
    "    This function gives back a 2D density plot \n",
    "    of the data put in as X and Y with \n",
    "    all points as scatter below certain density\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #first make sure to only use finite X and Y values\n",
    "    XY_finite = (np.isfinite(X) & np.isfinite(Y))\n",
    "    X = X[XY_finite]\n",
    "    Y = Y[XY_finite]\n",
    "    \n",
    "    # General kwargs:\n",
    "    xlabel = kwargs.get('xlabel','')\n",
    "    ylabel = kwargs.get('ylabel', r'$\\mathrm{[X/Fes]~[dex]}$')\n",
    "    xlim   = kwargs.get('xlim', (-3.0,0.65))\n",
    "    ylim   = kwargs.get('ylim', (-0.5,1.00))\n",
    "    cmap = kwargs.get('cmap', parula)\n",
    "    bins = kwargs.get('bins', (0.05,0.025))\n",
    "    if np.shape(bins) != ():\n",
    "        # assuming input in dex\n",
    "        bins = [np.arange(xlim[0],xlim[1],bins[0]),np.arange(ylim[0],ylim[1],bins[1])]\n",
    "    \n",
    "    # plot all points as scatter before density structure is overlaid\n",
    "    scatter = ab_scatter(X,Y,ax=ax)\n",
    "\n",
    "    H, xedges, yedges = np.histogram2d(X,Y,bins=bins)\n",
    "    H=np.rot90(H)\n",
    "    H=np.flipud(H)\n",
    "    Hmasked = np.ma.masked_where(H<min_per_bin,H)\n",
    "\n",
    "    dens2d=ax.pcolormesh(xedges,yedges,Hmasked,cmap=cmap)\n",
    "\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    xticks = kwargs.get('xticks',ax.get_xticks())\n",
    "    ax.set_xticks(xticks)\n",
    "    \n",
    "    if zeroslines == True:\n",
    "        ax.axhline(0,c='k',lw=0.5)\n",
    "        ax.axvline(0,c='k',lw=0.5)\n",
    "        \n",
    "    if interimlines == True:\n",
    "        ax.axhline(0.1,c='k',linestyle='--')\n",
    "        ax.axhline(0.2,c='k',linestyle='--')\n",
    "        ax.axhline(0.3,c='k',linestyle='--')\n",
    "        ax.axhline(0.4,c='k',linestyle='--')\n",
    "        ax.axhline(0.5,c='k',linestyle='--')\n",
    "        ax.axhline(-0.1,c='k',linestyle='--')\n",
    "        ax.axhline(-0.2,c='k',linestyle='--')\n",
    "\n",
    "    if colorbar == True:\n",
    "        c = plt.colorbar(dens2d,ax=ax)\n",
    "        c.set_label('Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_alpha(dictionary, sme_cannon='cannon', savefig=False, xlim=(-3.0,0.65), ylim=(-0.5,1.00)):\n",
    "    \n",
    "    interimlines = False#True\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    for each,mode in enumerate(['Alpha_fe','O','Mg','Si','Ca','Ti']):\n",
    "        ax = plt.subplot(2,3,each+1)\n",
    "        if each == 0:\n",
    "            X = dictionary['Feh_'+sme_cannon]\n",
    "            Y = dictionary['Alpha_fe_'+sme_cannon]\n",
    "        else:\n",
    "            good = dictionary['flag_'+mode+'_abund_'+sme_cannon] == 0\n",
    "            X = dictionary['Feh_'+sme_cannon][good]\n",
    "            Y = dictionary[mode+'_abund_'+sme_cannon][good]\n",
    "        ab_dens2d(\n",
    "            ax=ax,\n",
    "            xlim=xlim,\n",
    "            ylim=ylim,\n",
    "            bins=(0.1,0.05),\n",
    "            X = X,\n",
    "            Y = Y,\n",
    "            xlabel='',\n",
    "            ylabel='',\n",
    "            interimlines=interimlines\n",
    "            );\n",
    "        ax.text(0.05,0.1,tex_dict[mode],transform=ax.transAxes)\n",
    "        if each in [0,3]:\n",
    "            ax.set_ylabel(tex_dict['XFe'])\n",
    "        if each < 3:\n",
    "            ax.set_xticks([])\n",
    "        if each > 2:\n",
    "            ax.set_xlabel(tex_dict['Feh'])\n",
    "        ax.set_xticks(ax.get_xticks()[::2])\n",
    "        ax.set_yticks(ax.get_yticks()[::2])\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if savefig!=False:\n",
    "        plt.savefig(savefig+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_SME_Cannon_HRD(dictionary, savefig=False, xlim=(7900,3600), ylim=(5.,0.)):\n",
    "    \"\"\"\n",
    "    This routine plots 2 subplots with the SME HRD on the left and the Cannon HRD on the right\n",
    "    \n",
    "    INPUT:\n",
    "    dictionary : with keywords \n",
    "                 Teff_sme, Logg_sme, Feh_sme as well as Teff_cannon, Logg_cannon, Feh_cannon   \n",
    "    \n",
    "    \"\"\"\n",
    "    HRD_kwargs = dict(cmap=parula, s=10, lw=0.1, rasterized=True)\n",
    "    f, (ax1,ax2) = plt.subplots(1,2,figsize=(2*6.4, 4.8))\n",
    "\n",
    "    plot_isochrones(ax=ax1)\n",
    "    plot_isochrones(ax=ax2)\n",
    "    \n",
    "    s1 = ax1.scatter(dictionary['Teff_sme'],dictionary['Logg_sme'],c=dictionary['Feh_sme'], vmin=-2.5, vmax=0.5,**HRD_kwargs)\n",
    "    ax1.set_xlabel(tex_dict['Teff']+r' SME')\n",
    "    ax1.set_ylabel(tex_dict['Logg']+r' SME')\n",
    "    c1 = plt.colorbar(s1, ax=ax1)\n",
    "    c1.set_label(tex_dict['Feh']+r' SME')\n",
    "    ax1.set_xlim(xlim)\n",
    "    ax1.set_ylim(ylim)\n",
    "\n",
    "    plot_contours(dictionary['Teff_sme'], dictionary['Logg_sme'], xlim=xlim, ylim=ylim, ax=ax1)\n",
    "    plot_contours(dictionary['Teff_cannon'], dictionary['Logg_cannon'], xlim=xlim, ylim=ylim, ax=ax2)\n",
    "    \n",
    "    s2 = ax2.scatter(dictionary['Teff_cannon'],dictionary['Logg_cannon'],c=dictionary['Feh_cannon'], vmin=-2.5, vmax=0.5,**HRD_kwargs)\n",
    "    ax2.set_xlabel(tex_dict['Teff']+r' The Cannon')\n",
    "    ax2.set_ylabel(tex_dict['Logg']+r' The Cannon')\n",
    "    c2 = plt.colorbar(s2, ax=ax2)\n",
    "    c2.set_label(tex_dict['Feh']+r' The Cannon')\n",
    "    ax2.set_xlim(xlim)\n",
    "    ax2.set_ylim(ylim)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if savefig!=False:\n",
    "        plt.savefig(savefig+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_alpha_fe(data_structure,sme_cannon='cannon',alpha_elements=['O','Mg','Si','Ca','Ti']):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    \n",
    "    data_structure : Structure, from which the alpha-process elements come from\n",
    "                     This structure also has to have the keys \n",
    "                     'Alpha_fe_'+sme_cannon and 'e_Alpha_fe_'+sme_cannon\n",
    "    sme_cannon     : 'sme' or 'cannon' which will be use as key within data_structure\n",
    "                     default: 'cannon'\n",
    "    alpha_elements : the array including which alpha-process elements shall be included\n",
    "                     default: ['O','Mg','Si','Ca','Ti']\n",
    "    \n",
    "    OUTPUT:\n",
    "    \n",
    "    None, but the following entries will be filled:\n",
    "    data_structure['Alpha_fe_'+sme_cannon] & data_structure['e_Alpha_fe_'+sme_cannon] \n",
    "    \n",
    "    \"\"\"\n",
    "    for each_sobject_id in range(len(data_structure['sobject_id'])):\n",
    "\n",
    "        \n",
    "        combi = np.nansum(np.concatenate([(data_structure[x+'_abund_'+sme_cannon][each_sobject_id]/data_structure['e_'+x+'_abund_'+sme_cannon][each_sobject_id]**2.)[(np.isfinite(data_structure[x+'_abund_'+sme_cannon][each_sobject_id]) & (data_structure['flag_'+x+'_abund_'+sme_cannon][each_sobject_id] == 0))] for x in alpha_elements]))/np.nansum(np.concatenate([1./data_structure['e_'+x+'_abund_'+sme_cannon][each_sobject_id][np.isfinite(data_structure[x+'_abund_'+sme_cannon][each_sobject_id]) & (data_structure['flag_'+x+'_abund_'+sme_cannon][each_sobject_id] == 0)]**2. for x in alpha_elements]))\n",
    "        e_combi = np.sqrt(1./np.nansum(np.concatenate([1./(data_structure['e_'+x+'_abund_'+sme_cannon][each_sobject_id])[np.isfinite(data_structure[x+'_abund_'+sme_cannon][each_sobject_id]) & (data_structure['flag_'+x+'_abund_'+sme_cannon][each_sobject_id] == 0)]**2. for x in alpha_elements])))\n",
    "\n",
    "        if np.isfinite(combi):\n",
    "            data_structure['Alpha_fe_'+sme_cannon][each_sobject_id] = combi\n",
    "            data_structure['e_Alpha_fe_'+sme_cannon][each_sobject_id] = e_combi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_alpha(dictionary, sme_cannon='cannon', savefig=False, xlim=(-3.0,0.65), ylim=(-0.5,1.00)):\n",
    "    \n",
    "    interimlines = False#True\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    for each,mode in enumerate(['Alpha_fe','O','Mg','Si','Ca','Ti']):\n",
    "        ax = plt.subplot(2,3,each+1)\n",
    "        if each == 0:\n",
    "            X = dictionary['Feh_'+sme_cannon]\n",
    "            Y = dictionary['Alpha_fe_'+sme_cannon]\n",
    "        else:\n",
    "            good = dictionary['flag_'+mode+'_abund_'+sme_cannon] == 0\n",
    "            X = dictionary['Feh_'+sme_cannon][good]\n",
    "            Y = dictionary[mode+'_abund_'+sme_cannon][good]\n",
    "        ab_dens2d(\n",
    "            ax=ax,\n",
    "            xlim=xlim,\n",
    "            ylim=ylim,\n",
    "            bins=(0.1,0.05),\n",
    "            X = X,\n",
    "            Y = Y,\n",
    "            xlabel='',\n",
    "            ylabel='',\n",
    "            interimlines=interimlines\n",
    "            );\n",
    "        ax.text(0.05,0.1,tex_dict[mode],transform=ax.transAxes)\n",
    "        if each in [0,3]:\n",
    "            ax.set_ylabel(tex_dict['XFe'])\n",
    "        if each < 3:\n",
    "            ax.set_xticks([])\n",
    "        if each > 2:\n",
    "            ax.set_xlabel(tex_dict['Feh'])\n",
    "        ax.set_xticks(ax.get_xticks()[::2])\n",
    "        ax.set_yticks(ax.get_yticks()[::2])\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if savefig!=False:\n",
    "        plt.savefig(savefig+'.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The class 'trainingset', where we want to put in all important data and routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class trainingset(object):\n",
    "    \"\"\" \n",
    "    trainingset is the major class for the important information on the trainingset.\n",
    "    \n",
    "    In the future, this should be executable without the trainingset class\n",
    "    \n",
    "    Initialise with:\n",
    "    obs_date      : Date of observation for the particular testset\n",
    "    wg3_wg4_setup : setup dictionary with all relevant data for IRAF/SME/CANNON\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, wg3_wg4_setup):\n",
    "        os.chdir(localFilePath)\n",
    "        if not hasattr(self,'galah_dict'):\n",
    "            self.galah_dict = create_galah_dict(wg3_wg4_setup)\n",
    "        self.version_cannon     = wg3_wg4_setup['version_cannon']\n",
    "        self.setup_cannon       = wg3_wg4_setup['setup_cannon']\n",
    "        self.nr_ccds_cannon     = wg3_wg4_setup['nr_ccds_cannon']\n",
    "        self.version_reduction  = wg3_wg4_setup['version_reduction']\n",
    "        self.reduction_DR       = wg3_wg4_setup['reduction_DR']\n",
    "        self.version_sme        = wg3_wg4_setup['version_sme']\n",
    "        self.stellar_parameters = wg3_wg4_setup['stellar_parameters']\n",
    "        self.elements           = wg3_wg4_setup['elements']\n",
    "        \n",
    "        os.chdir('CANNON/'+self.reduction_DR+'/'+self.version_cannon)\n",
    "        print('You are now working in:'+os.getcwd())\n",
    "\n",
    "        self.SP_model_file         = self.version_cannon+'_Sp_'+self.setup_cannon+'_model.pickle'\n",
    "        self.SP_selftest_file      = self.version_cannon+'_Sp_'+self.setup_cannon+'_selftest_tags.pickle'\n",
    "        self.SP_training_data_file = self.version_cannon+'_Sp_'+self.setup_cannon+'_training_data.pickle'\n",
    "        self.SP_training_data_fits = 'trainingset/'+self.version_cannon+'_Sp_'+self.setup_cannon+'_trainingset.fits'\n",
    "        print(glob.glob(self.SP_selftest_file))\n",
    "        \n",
    "        self.AB_model_file         = {}\n",
    "        self.AB_selftest_file      = {}\n",
    "        self.AB_training_data_file = {}\n",
    "        self.AB_training_data_fits = {}\n",
    "\n",
    "        if self.elements!='None':\n",
    "            for each_element in self.elements:\n",
    "                try:\n",
    "                    print(glob.glob(self.version_cannon+'_'+each_element+'_'+self.setup_cannon+'_selftest_tags.pickle'))\n",
    "                    self.AB_model_file[each_element]         = self.version_cannon+'_'+each_element+'_'+self.setup_cannon+'_model.pickle'\n",
    "                    self.AB_selftest_file[each_element]      = self.version_cannon+'_'+each_element+'_'+self.setup_cannon+'_selftest_tags.pickle'\n",
    "                    self.AB_training_data_file[each_element] = self.version_cannon+'_'+each_element+'_'+self.setup_cannon+'_training_data.pickle'\n",
    "                    self.AB_training_data_fits[each_element] = 'trainingset/'+self.version_cannon+'_'+each_element+'_'+self.setup_cannon+'_trainingset.fits'\n",
    "                    print('Found automatic setup configuration for '+each_element+': '+found_setup[-1])\n",
    "                except:\n",
    "                    print('There is no setup configuration for '+each_element)\n",
    "                    \n",
    "\n",
    "    def initiate_cannon_structure(self,nstars):\n",
    "        if not hasattr(self,'cannon'):\n",
    "            print('cannon structure does not exist yet - initialising cannon structure now')\n",
    "            self.cannon = {}\n",
    "            for each_key in self.galah_dict.keys():\n",
    "                self.cannon[each_key] = np.array([self.galah_dict[each_key] for x in range(nstars)])\n",
    "        else: \n",
    "            print('You want to initialise cannon structure, but it already exists!')\n",
    "\n",
    "    def get_SME(self):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "        None\n",
    "        \n",
    "        OUTPUT:\n",
    "        Adds all important information from SME file\n",
    "        \"\"\"\n",
    "        \n",
    "        sme=pyfits.getdata(self.SP_training_data_fits)\n",
    "        if not hasattr(self,'cannon'):\n",
    "            self.initiate_cannon_structure(len(sme.sobject_id))\n",
    "        for each_key in sme.dtype.names:\n",
    "            self.cannon[each_key] = sme[each_key]\n",
    "            \n",
    "        compute_alpha_fe(self.cannon,sme_cannon='sme',alpha_elements=['O','Mg','Si','Ca','Ti'])\n",
    "\n",
    "    def get_SP(self):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "        None\n",
    "        \n",
    "        OUTPUT:\n",
    "        Adds all important information from stellar parameter run of the Cannon\n",
    "        \"\"\"\n",
    "        \n",
    "        if not hasattr(self,'cannon'):\n",
    "            sys.exit('self.cannon not yet initiated!')\n",
    "            \n",
    "        params, e_params, covs, chi2, sobject_id, chi2good, chi2each = read_model_pickle(self.SP_selftest_file)\n",
    "        if len(sobject_id) != len(self.cannon['sobject_id']):\n",
    "            sys.exit('Trainingset FITS and Cannon selftest do not have same length!')\n",
    "        else:\n",
    "            for index, each_param in enumerate(self.stellar_parameters):\n",
    "                self.cannon[each_param+'_cannon'] = params[:,index]\n",
    "                self.cannon['e_'+each_param+'_cannon'] = e_params[:,index]\n",
    "            self.cannon['chi2_cannon'] = chi2good\n",
    "\n",
    "    def get_AB(self):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "        None, but class attribute 'cannon' must exist!\n",
    "        \n",
    "        OUTPUT:\n",
    "        Adds all important information from the element abundance run of the Cannon to the class\n",
    "        \"\"\"\n",
    "        if hasattr(self,'cannon'):\n",
    "            for each_element in self.elements:\n",
    "                try:\n",
    "                    params, e_params, covs, chi2, sobject_ids, chi2good, chi2each = read_model_pickle(self.AB_data[each_element])\n",
    "                    self.cannon[each_element+'_abund_cannon'] = params[:,-1]\n",
    "                    self.cannon['e_'+each_element+'_abund_cannon'] = e_params[:,-1]\n",
    "                    # Here is the place to build in the function to compare with trainingset[each_element] for label distance\n",
    "                except:\n",
    "                    print('Element '+each_element+' not available!')\n",
    "                    self.cannon[each_element+'_abund_cannon'][:] = np.nan\n",
    "                    self.cannon['e_'+each_element+'_abund_cannon'][:] = np.nan\n",
    "                    self.cannon['flag_'+each_element+'_abund_cannon'][:] = -1\n",
    "                    self.cannon['ld_'+each_element+'_abund_cannon'][:] = np.nan\n",
    "                    pass\n",
    "            try:\n",
    "                compute_alpha_fe(self.cannon,sme_cannon='cannon',alpha_elements=['O','Mg','Si','Ca','Ti'])\n",
    "            except:\n",
    "                print('Could not compute Alpha_fe_cannon with the available elements')\n",
    "        else:\n",
    "            print('You did not call testset.get_SP() yet, which initialises the CANNON attribute to save the abundances in!')\n",
    "            \n",
    "    def get_IRAF(self):\n",
    "        if hasattr(self,'cannon'):\n",
    "            print('Reading in IRAF reduction version: '+self.version_reduction)\n",
    "            iraf = pyfits.getdata(localFilePath+'/DATA/'+self.version_reduction+'.fits',ext=1)\n",
    "            for cannon_index,each_sobject_id in enumerate(self.cannon['sobject_id']):\n",
    "                iraf_equivalent = np.where(each_sobject_id == iraf['sobject_id'])[0]\n",
    "                if len(iraf_equivalent) > 0:\n",
    "                    for each_iraf_label in [\n",
    "                            'galah_id','field_id','ra','dec','ebv',\n",
    "                            'snr_c1_iraf','snr_c2_iraf','snr_c3_iraf','snr_c4_iraf',\n",
    "                            'teff_guess','logg_guess','feh_guess','rv_guess','e_rv_guess','flag_guess'\n",
    "                            ]:\n",
    "                        self.cannon[each_iraf_label][cannon_index] = iraf[each_iraf_label][iraf_equivalent[0]]\n",
    "                else:\n",
    "                    print('Something went wrong here, because there is no IRAF entry for: '+str(each_sobject_id))\n",
    "        else:\n",
    "            print('You did not call testset.get_SP() yet, which initialises the CANNON attribute to save the abundances in!')\n",
    "\n",
    "    def export_fits(self):\n",
    "        export_fits(\n",
    "            self.cannon, \n",
    "            filename=localFilePath+'CANNON/'+self.reduction_DR+'/'+self.version_cannon+'/fits_files/'+self.version_cannon+'_'+self.setup_cannon+'_'+self.reduction_DR+'_'+str(self.nr_ccds_cannon)+'ccds_trainingset'\n",
    "        )\n",
    "\n",
    "    def plot_HRD(self, savefig=False):\n",
    "        plot_HRD(self.cannon, sme_cannon='cannon', savefig=savefig)\n",
    "        \n",
    "    def plot_alpha(self, savefig=False):\n",
    "        plot_alpha(self.cannon, sme_cannon='cannon', savefig=savefig)\n",
    "        \n",
    "    def plot_SME_Cannon_comparisons(self, savefig=False):\n",
    "        plot_SME_Cannon_HRD(self.cannon, savefig=savefig)\n",
    "        #plot_SME_Cannon_abundances(self.cannon, savefig=savefig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running the Cannon in IPYNB mode\n",
      "Cannon3.0.1 dr5.2 131119\n"
     ]
    }
   ],
   "source": [
    "if sys.argv[1] == '-f':\n",
    "    print('You are running the Cannon in IPYNB mode')\n",
    "    version_cannon = 'Cannon3.0.1'\n",
    "    reduction_DR   = 'dr5.2'\n",
    "    obs_date       = 131119\n",
    "    print(version_cannon,reduction_DR,obs_date)\n",
    "else:\n",
    "    print('You are running the Cannon in PY mode')\n",
    "    version_cannon = sys.argv[1]\n",
    "    reduction_DR   = sys.argv[2]\n",
    "    obs_date       = sys.argv[3]\n",
    "\n",
    "wg3_wg4_setup = dict(\n",
    "    version_reduction  = 'sobject_iraf_52_171009',\n",
    "    reduction_DR       = reduction_DR,\n",
    "    version_sme        = 'SME360_DR2_SVN331',\n",
    "    version_cannon     = version_cannon,\n",
    "    setup_cannon       = 'SMEmasks_it1',\n",
    "    nr_ccds_cannon     = 4,\n",
    "    stellar_parameters = ['Teff', 'Logg', 'Feh', 'Vmic', 'Vsini', 'Ak'],\n",
    "#     elements           = [\n",
    "#                          'Li',  'C',  'O', 'Na', 'Mg', 'Al', 'Si',  'K', 'Ca', 'Sc', \n",
    "#                          'Ti',  'V', 'Cr', 'Mn', 'Co', 'Ni', 'Cu', 'Zn', 'Rb', 'Sr',\n",
    "#                           'Y', 'Zr', 'Mo', 'Ru', 'Ba', 'La', 'Ce', 'Nd', 'Sm', 'Eu'\n",
    "#                          ]\n",
    "    elements           = [\n",
    "                         'Li'\n",
    "                         ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create the class for the trainingset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are now working in:/shared-storage/buder/svn-repos/trunk/GALAH/CANNON/dr5.2/Cannon3.0.1\n",
      "['Cannon3.0.1_Sp_SMEmasks_it1_selftest_tags.pickle']\n",
      "[]\n",
      "There is no setup configuration for Li\n"
     ]
    }
   ],
   "source": [
    "sme_ts = trainingset(wg3_wg4_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannon structure does not exist yet - initialising cannon structure now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buder/.local/lib/python2.7/site-packages/ipykernel/__main__.py:22: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/buder/.local/lib/python2.7/site-packages/ipykernel/__main__.py:23: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "sme_ts.get_SME()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will read pickle file: Cannon3.0.1_Sp_SMEmasks_it1_selftest_tags.pickle\n"
     ]
    }
   ],
   "source": [
    "sme_ts.get_SP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element Li not available!\n",
      "Could not compute Alpha_fe_cannon with the available elements\n"
     ]
    }
   ],
   "source": [
    "sme_ts.get_AB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sme_ts.export_fits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sme_ts.plot_SME_Cannon_comparisons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert IPYNB to PY\n",
    "\n",
    "os.chdir('/shared-storage/buder/svn-repos/trunk/GALAH/TheGALAHCannon/')\n",
    "\n",
    "convert_command = 'ipython nbconvert --to script Cannon_collect_trainingset.ipynb'\n",
    "os.system(convert_command)\n",
    "\n",
    "os.chdir('/shared-storage/buder/svn-repos/trunk/GALAH/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
